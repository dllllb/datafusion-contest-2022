{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "simplified-country",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/kireev/pycharm-deploy/vtb\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a60312c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "joined-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "restricted-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d326f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "subtle-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "oriented-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "labeled-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhocon import ConfigFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "outer-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "monthly-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.data_load.iterable_processing.category_size_clip import CategorySizeClip\n",
    "from dltranz.data_load import augmentation_chain\n",
    "from dltranz.data_load.augmentations.seq_len_limit import SeqLenLimit\n",
    "from dltranz.data_load.augmentations.random_slice import RandomSlice\n",
    "\n",
    "from dltranz.seq_encoder import create_encoder\n",
    "\n",
    "from dltranz.metric_learn.sampling_strategies import get_sampling_strategy\n",
    "from dltranz.metric_learn.losses import get_loss\n",
    "\n",
    "from dltranz.tb_interface import get_scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "latter-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vtb_code.data import PairedDataset, paired_collate_fn, PairedZeroDataset, DropDuplicate\n",
    "from vtb_code.metrics import PrecisionK, MeanReciprocalRankK, ValidationCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-ready",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "accepted-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD_ID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "under-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_id_test = FOLD_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fifty-waste",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds_count = len(glob('data/train_matching_*.csv'))\n",
    "folds_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "suburban-detector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fold_id_valid = np.random.choice([i for i in range(folds_count) if i != fold_id_test], size=1)[0]\n",
    "fold_id_valid = (fold_id_test + 1) % folds_count\n",
    "fold_id_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "confidential-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matching_train = pd.concat([pd.read_csv(f'data/train_matching_{i}.csv')\n",
    "                              for i in range(folds_count) \n",
    "                              if i not in (fold_id_test, fold_id_valid)])\n",
    "df_matching_valid = pd.read_csv(f'data/train_matching_{fold_id_valid}.csv')\n",
    "df_matching_test = pd.read_csv(f'data/train_matching_{fold_id_test}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "smooth-experiment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11721, 2930, 2930]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(df) for df in [df_matching_train, df_matching_valid, df_matching_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "toxic-defeat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.01 s, sys: 3.45 s, total: 12.5 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(f'data/features_f{FOLD_ID}.pickle', 'rb') as f:\n",
    "    (\n",
    "        features_trx_train,\n",
    "        features_trx_valid,\n",
    "        features_trx_test,\n",
    "        features_trx_puzzle,\n",
    "        features_click_train,\n",
    "        features_click_valid,\n",
    "        features_click_test,\n",
    "        features_click_puzzle,\n",
    "    ) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b0062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a652e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da05d450",
   "metadata": {},
   "source": [
    "# Preetrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed1be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_mlm_trx = torch.utils.data.DataLoader(\n",
    "    PairedDataset(\n",
    "        np.sort(np.array(list(features_trx_train.keys()))).reshape(-1, 1),\n",
    "        data=[features_trx_train],\n",
    "        augmentations=[augmentation_chain(\n",
    "            DropDuplicate('mcc_code', col_new_cnt='c_cnt'), \n",
    "            RandomSlice(32, 128)\n",
    "        )],\n",
    "        n_sample=1,\n",
    "    ),\n",
    "    collate_fn=paired_collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=12,\n",
    "    batch_size=128,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "valid_dl_mlm_trx = torch.utils.data.DataLoader(\n",
    "    PairedDataset(\n",
    "        np.sort(np.array(list(features_trx_test.keys()))).reshape(-1, 1),\n",
    "        data=[features_trx_test],\n",
    "        augmentations=[augmentation_chain(\n",
    "            DropDuplicate('mcc_code', col_new_cnt='c_cnt'), \n",
    "            RandomSlice(32, 128)\n",
    "        )],\n",
    "        n_sample=1,\n",
    "    ),\n",
    "    collate_fn=paired_collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=12,\n",
    "    batch_size=128,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d69ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_mlm_click = torch.utils.data.DataLoader(\n",
    "    PairedDataset(\n",
    "        np.sort(np.array(list(features_click_train.keys()))).reshape(-1, 1),\n",
    "        data=[features_click_train],\n",
    "        augmentations=[augmentation_chain(\n",
    "            DropDuplicate('cat_id', col_new_cnt='c_cnt'), \n",
    "            RandomSlice(32, 128)\n",
    "        )],\n",
    "        n_sample=1,\n",
    "    ),\n",
    "    collate_fn=paired_collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=12,\n",
    "    batch_size=128,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "valid_dl_mlm_click = torch.utils.data.DataLoader(\n",
    "    PairedDataset(\n",
    "        np.sort(np.array(list(features_click_test.keys()))).reshape(-1, 1),\n",
    "        data=[features_click_test],\n",
    "        augmentations=[augmentation_chain(\n",
    "            DropDuplicate('cat_id', col_new_cnt='c_cnt'), \n",
    "            RandomSlice(32, 128)\n",
    "        )],\n",
    "        n_sample=1,\n",
    "    ),\n",
    "    collate_fn=paired_collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=12,\n",
    "    batch_size=128,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bdd76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a177ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = []\n",
    "for batch in train_dl_mlm_trx:\n",
    "    v.append(batch[0][0].payload['transaction_amt'][batch[0][0].seq_len_mask.bool()])\n",
    "v = torch.cat(v)\n",
    "\n",
    "trx_amnt_quantiles = torch.quantile(torch.unique(v), torch.linspace(0, 1, 100))\n",
    "trx_amnt_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f874c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8b98778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.trx_encoder import TrxEncoder, PaddedBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "289c6011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vtb_code.models import MeanLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "mechanical-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrxTransform(torch.nn.Module):\n",
    "    def __init__(self, trx_amnt_quantiles):\n",
    "        super().__init__()\n",
    "        self.trx_amnt_quantiles = torch.nn.Parameter(trx_amnt_quantiles, requires_grad=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x.payload['transaction_amt_q'] = torch.bucketize(x.payload['transaction_amt'], self.trx_amnt_quantiles) + 1\n",
    "        return x\n",
    "    \n",
    "class CustomClickTransform(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "#         x.payload['cat_id'] = torch.clamp(x.payload['cat_id'], 0, 300)\n",
    "#         x.payload['level_0'] = torch.clamp(x.payload['level_0'], 0, 200)\n",
    "#         x.payload['level_1'] = torch.clamp(x.payload['level_1'], 0, 200)\n",
    "#         x.payload['level_2'] = torch.clamp(x.payload['level_2'], 0, 200)\n",
    "#         x.payload['c_cnt_clamp'] = torch.clamp(x.payload['c_cnt'], 0, 20).int()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "395037bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateFeaturesTransform(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        et = x.payload['event_time'].int()\n",
    "        et_day = et.div(24 * 60 * 60, rounding_mode='floor').int()\n",
    "        x.payload['hour'] = et.div(60 * 60, rounding_mode='floor') % 24 + 1\n",
    "        x.payload['weekday'] = et.div(60 * 60 * 24, rounding_mode='floor') % 7 + 1\n",
    "        x.payload['day_diff'] = torch.clamp(torch.diff(et_day, prepend=et_day[:, :1], dim=1), 0, 14)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1d9e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBLinear(torch.nn.Linear):\n",
    "    def forward(self, x: PaddedBatch):\n",
    "        return PaddedBatch(super().forward(x.payload), x.seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed9272aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBL2Norm(torch.nn.Module):\n",
    "    def __init__(self, beta):    \n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return PaddedBatch(self.beta * x.payload / (x.payload.pow(2).sum(dim=-1, keepdim=True) + 1e-9).pow(0.5), \n",
    "                           x.seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "952d0fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vtb_code.data import frequency_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6396ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMPretrainModule(pl.LightningModule):\n",
    "    def __init__(self, data_type, params,\n",
    "                 lr, weight_decay,\n",
    "                 max_lr, pct_start, total_steps,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        common_trx_size = params['common_trx_size']\n",
    "        self.seq_encoder = None\n",
    "        \n",
    "        self.token_mask = torch.nn.Parameter(torch.randn(1, 1, common_trx_size), requires_grad=True)\n",
    "        self.transf = torch.nn.TransformerEncoder(\n",
    "            encoder_layer=torch.nn.TransformerEncoderLayer(\n",
    "                d_model=common_trx_size,\n",
    "                nhead=params['transf.nhead'],\n",
    "                dim_feedforward=params['transf.dim_feedforward'],\n",
    "                dropout=params['transf.dropout'],\n",
    "                batch_first=True,\n",
    "            ),\n",
    "            num_layers=params['transf.num_layers'], \n",
    "            norm=torch.nn.LayerNorm(common_trx_size) if params['transf.norm'] else None,\n",
    "        )\n",
    "        \n",
    "        if params['transf.use_pe']:\n",
    "            self.pe = torch.nn.Parameter(self.get_pe(), requires_grad=False)\n",
    "        else:\n",
    "            self.pe = None\n",
    "        self.padding_mask = torch.nn.Parameter(torch.tensor([True, False]).bool(), requires_grad=False)\n",
    "\n",
    "        self.train_mlm_loss_all = MeanLoss(compute_on_step=False)\n",
    "        self.valid_mlm_loss_all = MeanLoss(compute_on_step=False)\n",
    "        self.train_mlm_loss_self = MeanLoss(compute_on_step=False)\n",
    "        self.valid_mlm_loss_self = MeanLoss(compute_on_step=False)\n",
    "        \n",
    "    def get_pe(self):\n",
    "        max_len = self.hparams.params['transf.max_len']\n",
    "        H = self.hparams.params['common_trx_size']\n",
    "        f = 2 * np.pi * torch.arange(max_len).view(1, -1, 1) / \\\n",
    "        torch.exp(torch.linspace(*np.log([4, max_len]), H // 2)).view(1, 1, -1)\n",
    "        return torch.cat([torch.sin(f), torch.cos(f)], dim=2)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer=optim,\n",
    "            max_lr=self.hparams.max_lr,\n",
    "            total_steps=self.hparams.total_steps,\n",
    "            pct_start=self.hparams.pct_start,\n",
    "            anneal_strategy='cos',\n",
    "            cycle_momentum=False,\n",
    "            div_factor=25.0,\n",
    "            final_div_factor=10000.0,\n",
    "            three_phase=True,\n",
    "        )\n",
    "        scheduler = {'scheduler': scheduler, 'interval': 'step'}\n",
    "        return [optim], [scheduler]\n",
    "            \n",
    "    def get_mask(self, x: PaddedBatch):\n",
    "        return torch.bernoulli(x.seq_len_mask.float() * self.hparams.params['mlm.replace_proba']).bool()\n",
    "        \n",
    "    def mask_x(self, x: PaddedBatch, mask):\n",
    "        return torch.where(mask.unsqueeze(2).expand_as(x.payload), \n",
    "                           self.token_mask.expand_as(x.payload), x.payload)\n",
    "        \n",
    "    def get_neg_ix(self, mask, neg_type):\n",
    "        \"\"\"Sample from predicts, where `mask == True`, without self element.\n",
    "        For `neg_type='all'` - sample from predicted tokens from batch\n",
    "        For `neg_type='self'` - sample from predicted tokens from row\n",
    "        \"\"\"\n",
    "        if neg_type == 'all':\n",
    "            mn = mask.float().view(1, -1) - \\\n",
    "                torch.eye(mask.numel(), device=mask.device)[mask.flatten()]\n",
    "            neg_ix = torch.multinomial(mn, self.hparams.params['mlm.neg_count_all'])\n",
    "            b_ix = neg_ix.div(mask.size(1), rounding_mode='trunc')\n",
    "            neg_ix = neg_ix % mask.size(1)\n",
    "            return b_ix, neg_ix\n",
    "        if neg_type == 'self':\n",
    "            mask_ix = mask.nonzero(as_tuple=False)\n",
    "            one_pos = torch.eye(mask.size(1), device=mask.device)[mask_ix[:, 1]]\n",
    "            mn = mask[mask_ix[:, 0]].float() - one_pos\n",
    "            mn = mn + 1e-9 * (1 - one_pos)\n",
    "            neg_ix = torch.multinomial(mn, self.hparams.params['mlm.neg_count_self'], replacement=True)\n",
    "            b_ix = mask_ix[:, 0].view(-1, 1).expand_as(neg_ix)\n",
    "            return b_ix, neg_ix\n",
    "        raise AttributeError(f'Unknown neg_type: {neg_type}')\n",
    "    \n",
    "    def sentence_encoding(self, x: PaddedBatch):\n",
    "        return None\n",
    "        \n",
    "    def mlm_loss(self, x: PaddedBatch, neg_type, x_orig: PaddedBatch):\n",
    "        mask = self.get_mask(x)\n",
    "        masked_x = self.mask_x(x, mask)\n",
    "        B, T, H = masked_x.size()\n",
    "        \n",
    "        if self.pe is not None:\n",
    "            if self.training:\n",
    "                start_pos = np.random.randint(0, self.hparams.params['transf.max_len'] - T, 1)[0]\n",
    "            else:\n",
    "                start_pos = 0\n",
    "            pe = self.pe[:, start_pos:start_pos + T]\n",
    "            masked_x = masked_x + pe\n",
    "            \n",
    "        se = self.sentence_encoding(x_orig)\n",
    "        if se is not None:\n",
    "            masked_x = masked_x + se\n",
    "\n",
    "        out = self.transf(masked_x, src_key_padding_mask=self.padding_mask[x.seq_len_mask])\n",
    "        \n",
    "        if self.pe is not None:\n",
    "            out = out - pe\n",
    "        if se is not None:\n",
    "            out = out - se\n",
    "\n",
    "        target = x.payload[mask].unsqueeze(1)  # N, 1, H\n",
    "        predict = out[mask].unsqueeze(1) # N, 1, H\n",
    "        neg_ix = self.get_neg_ix(mask, neg_type)\n",
    "        negative = out[neg_ix[0], neg_ix[1]]  # N, nneg, H\n",
    "        out_samples = torch.cat([predict, negative], dim=1)\n",
    "        probas = torch.softmax((target * out_samples).sum(dim=2), dim=1)\n",
    "        loss = -torch.log(probas[:, 0])\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x_trx, _), = batch\n",
    "        \n",
    "        z_trx = self.seq_encoder(x_trx)  # PB: B, T, H\n",
    "        \n",
    "        loss_mlm = self.mlm_loss(z_trx, neg_type='all', x_orig=x_trx)\n",
    "        self.train_mlm_loss_all(loss_mlm)\n",
    "        loss_mlm_all = loss_mlm.mean()\n",
    "        self.log(f'loss/mlm_{self.hparams.data_type}', loss_mlm_all)\n",
    "\n",
    "        loss_mlm = self.mlm_loss(z_trx, neg_type='self', x_orig=x_trx)\n",
    "        self.train_mlm_loss_self(loss_mlm)\n",
    "        loss_mlm_self = loss_mlm.mean()\n",
    "        self.log(f'loss/mlm_{self.hparams.data_type}_self', loss_mlm_self)\n",
    "        \n",
    "        return loss_mlm_all + loss_mlm_self\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        (x_trx, _), = batch\n",
    "        z_trx = self.seq_encoder(x_trx)  # PB: B, T, H\n",
    "        \n",
    "        loss_mlm = self.mlm_loss(z_trx, neg_type='all', x_orig=x_trx)\n",
    "        self.valid_mlm_loss_all(loss_mlm)\n",
    "        \n",
    "        loss_mlm = self.mlm_loss(z_trx, neg_type='self', x_orig=x_trx)\n",
    "        self.valid_mlm_loss_self(loss_mlm)\n",
    "        \n",
    "    def training_epoch_end(self, _):\n",
    "        self.log(f'metrics/train_{self.hparams.data_type}_mlm', self.train_mlm_loss_all, prog_bar=False)\n",
    "        self.log(f'metrics/train_{self.hparams.data_type}_mlm_self', self.train_mlm_loss_self, prog_bar=False)\n",
    "        \n",
    "    def validation_epoch_end(self, _):\n",
    "        self.log(f'metrics/valid_{self.hparams.data_type}_mlm', self.valid_mlm_loss_all, prog_bar=True)\n",
    "        self.log(f'metrics/valid_{self.hparams.data_type}_mlm_self', self.valid_mlm_loss_self, prog_bar=True)\n",
    "\n",
    "        \n",
    "class MLMPretrainModuleTrx(MLMPretrainModule):\n",
    "    def __init__(self,\n",
    "                 trx_amnt_quantiles, \n",
    "                 params,\n",
    "                 lr, weight_decay,\n",
    "                 max_lr, pct_start, total_steps,\n",
    "                ):\n",
    "        super().__init__(data_type='trx',\n",
    "                         params=params,\n",
    "                         lr=lr, weight_decay=weight_decay,\n",
    "                         max_lr=max_lr, pct_start=pct_start, total_steps=total_steps,\n",
    "                        )\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        common_trx_size = self.hparams.params['common_trx_size']\n",
    "        t = TrxEncoder(self.hparams.params['trx_seq.trx_encoder'])\n",
    "        self.seq_encoder = torch.nn.Sequential(\n",
    "            CustomTrxTransform(trx_amnt_quantiles=trx_amnt_quantiles),\n",
    "            DateFeaturesTransform(),\n",
    "            t, PBLinear(t.output_size, common_trx_size),\n",
    "            PBL2Norm(self.hparams.params['mlm.beta']),\n",
    "        )\n",
    "        \n",
    "\n",
    "class MLMPretrainModuleClick(MLMPretrainModule):\n",
    "    def __init__(self, params,\n",
    "                 lr, weight_decay,\n",
    "                 max_lr, pct_start, total_steps,\n",
    "                ):\n",
    "        super().__init__(data_type='click',\n",
    "                         params=params,\n",
    "                         lr=lr, weight_decay=weight_decay,\n",
    "                         max_lr=max_lr, pct_start=pct_start, total_steps=total_steps,\n",
    "                        )\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        common_trx_size = self.hparams.params['common_trx_size']\n",
    "        t = TrxEncoder(self.hparams.params['click_seq.trx_encoder'])\n",
    "        self.seq_encoder = torch.nn.Sequential(\n",
    "            CustomClickTransform(),\n",
    "            DateFeaturesTransform(),\n",
    "            t, PBLinear(t.output_size, common_trx_size),\n",
    "            PBL2Norm(self.hparams.params['mlm.beta']),\n",
    "        )\n",
    "        \n",
    "#         self.se = torch.nn.Embedding(7, common_trx_size, padding_idx=0)\n",
    "\n",
    "#     def sentence_encoding(self, x: PaddedBatch):\n",
    "#         se = torch.stack([frequency_encoder(v, m.bool())\n",
    "#                           for v, m in zip(x.payload['new_uid'], x.seq_len_mask)], dim=0).clamp(None, 6)\n",
    "#         return self.se(se)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a087409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigFactory.parse_string('''\n",
    "    common_trx_size: 256\n",
    "    transf: {\n",
    "        nhead: 4\n",
    "        dim_feedforward: 1024\n",
    "        dropout: 0.1\n",
    "        num_layers: 3\n",
    "        norm: false\n",
    "        max_len: 6000\n",
    "        use_pe: true\n",
    "    }\n",
    "    mlm: {\n",
    "        replace_proba: 0.1\n",
    "        neg_count_all: 64\n",
    "        neg_count_self: 8\n",
    "        beta: 10\n",
    "    }\n",
    "    trx_seq: {\n",
    "        trx_encoder: {\n",
    "          use_batch_norm_with_lens: false\n",
    "          norm_embeddings: false,\n",
    "          embeddings_noise: 0.000,\n",
    "          embeddings: {\n",
    "            mcc_code: {in: 350, out: 64},\n",
    "            currency_rk: {in: 10, out: 4}\n",
    "            transaction_amt_q: {in: 110, out: 8}\n",
    "            \n",
    "            hour: {in: 30, out: 16}\n",
    "            weekday: {in: 10, out: 4}\n",
    "            day_diff: {in: 15, out: 8}\n",
    "          },\n",
    "          numeric_values: {\n",
    "            transaction_amt: identity\n",
    "            c_cnt: log\n",
    "          }\n",
    "          was_logified: false\n",
    "          log_scale_factor: 1.0\n",
    "        },\n",
    "    }\n",
    "    click_seq: {\n",
    "        trx_encoder: {\n",
    "          use_batch_norm_with_lens: false\n",
    "          norm_embeddings: false,\n",
    "          embeddings_noise: 0.000,\n",
    "          embeddings: {\n",
    "            cat_id: {in: 400, out: 64},\n",
    "            level_0: {in: 400, out: 16}\n",
    "            level_1: {in: 400, out: 8}\n",
    "            level_2: {in: 400, out: 4}\n",
    "            \n",
    "            hour: {in: 30, out: 16}\n",
    "            weekday: {in: 10, out: 4}\n",
    "            day_diff: {in: 15, out: 8}\n",
    "          },\n",
    "          numeric_values: {\n",
    "            c_cnt: log\n",
    "          }\n",
    "          was_logified: false\n",
    "          log_scale_factor: 1.0\n",
    "        },\n",
    "    }\n",
    "''')\n",
    "\n",
    "mlm_model_trx = MLMPretrainModuleTrx(\n",
    "    params=config,                     \n",
    "    lr=0.001, weight_decay=0,\n",
    "    max_lr=0.001, pct_start=9000 / 2 / 10000, total_steps=10000,\n",
    "    trx_amnt_quantiles=trx_amnt_quantiles,\n",
    ")\n",
    "mlm_model_click = MLMPretrainModuleClick(\n",
    "    params=config,                     \n",
    "    lr=0.001, weight_decay=0,\n",
    "    max_lr=0.001, pct_start=9000 / 2 / 10000, total_steps=10000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcde695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b702285",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=[0],\n",
    "    max_steps=10000,\n",
    "    enable_progress_bar=False,\n",
    "    callbacks=[\n",
    "        pl.callbacks.LearningRateMonitor(),\n",
    "        pl.callbacks.ModelCheckpoint(\n",
    "            every_n_train_steps=2000, save_top_k=-1,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "model_version_trx = trainer.logger.version\n",
    "print('baseline all:  {:.3f}'.format(np.log(mlm_model_trx.hparams.params['mlm.neg_count_all'] + 1)))\n",
    "print('baseline self: {:.3f}'.format(np.log(mlm_model_trx.hparams.params['mlm.neg_count_self'] + 1)))\n",
    "print(f'version = {model_version_trx}')\n",
    "trainer.fit(mlm_model_trx, train_dl_mlm_trx, valid_dl_mlm_trx)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b739381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=[0],\n",
    "    max_steps=10000,\n",
    "    enable_progress_bar=False,\n",
    "    callbacks=[\n",
    "        pl.callbacks.LearningRateMonitor(),\n",
    "        pl.callbacks.ModelCheckpoint(\n",
    "            every_n_train_steps=2000, save_top_k=-1,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "model_version_click = trainer.logger.version\n",
    "print('baseline all:  {:.3f}'.format(np.log(mlm_model_click.hparams.params['mlm.neg_count_all'] + 1)))\n",
    "print('baseline self: {:.3f}'.format(np.log(mlm_model_click.hparams.params['mlm.neg_count_self'] + 1)))\n",
    "print(f'version = {model_version_click}')\n",
    "trainer.fit(mlm_model_click, train_dl_mlm_click, valid_dl_mlm_click)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146538b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82bfc3f4",
   "metadata": {},
   "source": [
    "# Use pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "392159ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version_trx = 148\n",
    "model_version_click = 149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c823acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "42, 43 - 256 without norm\n",
    "\n",
    "70, 71 - 256 without L2Norm and beta=10, neg_count_all=16\n",
    "\n",
    "72, 74 - 512 without L2Norm and beta=10, neg_count_all=16\n",
    "\n",
    "73, 75 - 512 without L2Norm and beta=10, neg_count_all=64\n",
    "\n",
    "_,  117 - 512 without L2Norm and beta=10, neg_count_all=64, with sentence_encoding\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "mlm_model_trx = MLMPretrainModuleTrx.load_from_checkpoint(\n",
    "    'lightning_logs/version_148/checkpoints/epoch=108-step=9999.ckpt')  # 42\n",
    "mlm_model_click = MLMPretrainModuleClick.load_from_checkpoint(\n",
    "    'lightning_logs/version_149/checkpoints/epoch=129-step=9999.ckpt')  # 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ca25b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlm_model_trx.freeze()\n",
    "# mlm_model_click.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-hartford",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "pursuant-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.seq_encoder.utils import NormEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c992ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.trx_encoder import TrxEncoder, PaddedBatch\n",
    "from dltranz.seq_encoder.rnn_encoder import RnnEncoder\n",
    "from dltranz.seq_encoder.utils import LastStepEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b11d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "crude-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2Scorer(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        B, H = x.size()\n",
    "        a, b =x[:, :H // 2], x[:, H // 2:]\n",
    "        return -(a - b).pow(2).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2698f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBLayerNorm(torch.nn.LayerNorm):\n",
    "    def forward(self, x: PaddedBatch):\n",
    "        return PaddedBatch(super().forward(x.payload), x.seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "lesbian-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedModule(pl.LightningModule):\n",
    "    def __init__(self, params, k,\n",
    "                 lr, weight_decay,\n",
    "                 max_lr, pct_start, total_steps,\n",
    "                 beta, neg_count,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        common_trx_size = mlm_model_trx.hparams.params['common_trx_size']\n",
    "        self.rnn_enc =  torch.nn.Sequential(\n",
    "            RnnEncoder(common_trx_size, params['rnn']), \n",
    "            LastStepEncoder(),\n",
    "#             NormEncoder(),\n",
    "        )\n",
    "        self._seq_encoder_trx = torch.nn.Sequential(\n",
    "            mlm_model_trx.seq_encoder,\n",
    "            PBLayerNorm(common_trx_size),\n",
    "        )\n",
    "        self._seq_encoder_click = torch.nn.Sequential(\n",
    "            mlm_model_click.seq_encoder,\n",
    "            PBLayerNorm(common_trx_size),\n",
    "        )\n",
    "        self.mlm_model_click = mlm_model_click\n",
    "        \n",
    "        self.cls = torch.nn.Sequential(\n",
    "            L2Scorer(),\n",
    "        )\n",
    "\n",
    "        self.train_precision = PrecisionK(k=k, compute_on_step=False)\n",
    "        self.train_mrr = MeanReciprocalRankK(k=k, compute_on_step=False)\n",
    "        self.valid_precision = PrecisionK(k=k, compute_on_step=False)\n",
    "        self.valid_mrr = MeanReciprocalRankK(k=k, compute_on_step=False)\n",
    "        \n",
    "    def seq_encoder_trx(self, x):\n",
    "        x = self._seq_encoder_trx(x)\n",
    "        return self.rnn_enc(x)\n",
    "    \n",
    "    def seq_encoder_click(self, x_orig):\n",
    "        x = self._seq_encoder_click(x_orig)\n",
    "#         x = PaddedBatch(\n",
    "#             x.payload + self.mlm_model_click.sentence_encoding(x_orig),\n",
    "#             x.seq_lens,\n",
    "#         )\n",
    "        return self.rnn_enc(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer=optim,\n",
    "            max_lr=self.hparams.max_lr,\n",
    "            total_steps=self.hparams.total_steps,\n",
    "            pct_start=self.hparams.pct_start,\n",
    "            anneal_strategy='cos',\n",
    "            cycle_momentum=False,\n",
    "            div_factor=25.0,\n",
    "            final_div_factor=10000.0,\n",
    "            three_phase=True,\n",
    "        )\n",
    "        scheduler = {'scheduler': scheduler, 'interval': 'step'}\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "    def loss_fn_p(self, embeddings, labels, ref_emb, ref_labels):\n",
    "        beta = self.hparams.beta\n",
    "        neg_count = self.hparams.neg_count\n",
    "        \n",
    "        pos_ix = (labels.view(-1, 1) == ref_labels.view(1, -1)).nonzero(as_tuple=False)\n",
    "        pos_labels = labels[pos_ix[:, 0]]\n",
    "        neg_w = ((pos_labels.view(-1, 1) != ref_labels.view(1, -1))).float()\n",
    "        neg_ix = torch.multinomial(neg_w, neg_count - 1)\n",
    "        all_ix = torch.cat([pos_ix[:, [1]], neg_ix], dim=1)\n",
    "        logits = -(embeddings[pos_ix[:, [0]]] - ref_emb[all_ix]).pow(2).sum(dim=2)\n",
    "        logits = logits * beta\n",
    "        logs = -torch.log(torch.softmax(logits, dim=1))[:, 0]\n",
    "#         logs = torch.relu(logs + np.log(0.1))\n",
    "        return logs.mean()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # pairs\n",
    "        x_trx, l_trx, m_trx, x_click, l_click, m_click = batch\n",
    "        z_trx = self.seq_encoder_trx(x_trx)  # B, H\n",
    "        z_click = self.seq_encoder_click(x_click)  # B, H\n",
    "        loss_pt = self.loss_fn_p(embeddings=z_trx, labels=l_trx, ref_emb=z_click, ref_labels=l_click)\n",
    "        self.log('loss/loss_pt', loss_pt)\n",
    "        \n",
    "        loss_pc = self.loss_fn_p(embeddings=z_click, labels=l_click, ref_emb=z_trx, ref_labels=l_trx)\n",
    "        self.log('loss/loss_pc', loss_pc)\n",
    "       \n",
    "        with torch.no_grad():\n",
    "            out = -(z_trx.unsqueeze(1) - z_click.unsqueeze(0)).pow(2).sum(dim=2)\n",
    "            out = out[m_trx == 0][:, m_click == 0]\n",
    "            T, C = out.size()\n",
    "            assert T == C\n",
    "            n_samples = z_trx.size(0) // (l_trx.max().item() + 1)\n",
    "            for i in range(n_samples):\n",
    "                l2 = out[i::n_samples, i::n_samples]\n",
    "                self.train_precision(l2)\n",
    "                self.train_mrr(l2)\n",
    "        \n",
    "        return loss_pt + 0.1 * loss_pc  #  loss_pc \n",
    "\n",
    "    def training_epoch_end(self, _):\n",
    "        self.log('train_metrics/precision', self.train_precision, prog_bar=True)\n",
    "        self.log('train_metrics/mrr', self.train_mrr, prog_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-conference",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-disaster",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5aff1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    PairedZeroDataset(\n",
    "        pd.concat([df_matching_train, df_matching_test], axis=0)[lambda x: x['rtk'].ne('0')].values,\n",
    "        data=[\n",
    "            dict(chain(features_trx_train.items(), features_trx_test.items())),\n",
    "            dict(chain(features_click_train.items(), features_click_test.items())),\n",
    "        ],\n",
    "        augmentations=[\n",
    "            augmentation_chain(DropDuplicate('mcc_code', col_new_cnt='c_cnt'), RandomSlice(32, 1024)),  # 1024\n",
    "            augmentation_chain(DropDuplicate('cat_id', col_new_cnt='c_cnt'), RandomSlice(64, 2048)),  # 2048\n",
    "        ],\n",
    "        n_sample=2,\n",
    "    ),\n",
    "    collate_fn=PairedZeroDataset.collate_fn,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=24,\n",
    "    batch_size=batch_size,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "valid_dl_trx = torch.utils.data.DataLoader(\n",
    "    PairedDataset(\n",
    "        np.sort(df_matching_valid['bank'].unique()).reshape(-1, 1), \n",
    "        data=[\n",
    "            features_trx_valid,\n",
    "        ],\n",
    "        augmentations=[\n",
    "            augmentation_chain(DropDuplicate('mcc_code', col_new_cnt='c_cnt'), SeqLenLimit(2000)),  # 2000\n",
    "        ],\n",
    "        n_sample=1,\n",
    "    ),\n",
    "    collate_fn=paired_collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    batch_size=128,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "valid_dl_click = torch.utils.data.DataLoader(\n",
    "    PairedDataset(\n",
    "        np.sort(df_matching_valid[lambda x: x['rtk'].ne('0')]['rtk'].unique()).reshape(-1, 1),\n",
    "        data=[\n",
    "            features_click_valid,\n",
    "        ],\n",
    "        augmentations=[\n",
    "            augmentation_chain(DropDuplicate('cat_id', col_new_cnt='c_cnt'), SeqLenLimit(5000)),  # 5000\n",
    "        ],\n",
    "        n_sample=1,\n",
    "    ),\n",
    "    collate_fn=paired_collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    batch_size=128,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "black-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_model = PairedModule(\n",
    "    ConfigFactory.parse_string('''\n",
    "    common_trx_size: 128\n",
    "    rnn: {\n",
    "      type: gru,\n",
    "      hidden_size: 256,\n",
    "      bidir: false,\n",
    "      trainable_starter: static\n",
    "    }\n",
    "'''),                     \n",
    "    k=100 * batch_size // 3000,\n",
    "    lr=0.0022, weight_decay=0,\n",
    "    max_lr=0.0018, pct_start=1100 / 6000, total_steps=6000,\n",
    "    beta=0.2 / 1.4, neg_count=120,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-brisbane",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecological-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationCallback(pl.Callback):\n",
    "    def __init__(self, v_trx, v_click, target, device, device_main, k=100, batch_size=1024):\n",
    "        self.v_trx = v_trx\n",
    "        self.v_click = v_click\n",
    "        self.target = target\n",
    "        self.device = device\n",
    "        self.device_main = device_main\n",
    "        self.k = k\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        was_traning = False\n",
    "        if pl_module.training:\n",
    "            pl_module.eval()\n",
    "            was_traning = True\n",
    "\n",
    "        pl_module.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            z_trx = []\n",
    "            for ((x_trx, _),) in self.v_trx:\n",
    "                z_trx.append(pl_module.seq_encoder_trx(x_trx.to(self.device)))\n",
    "            z_trx = torch.cat(z_trx, dim=0)\n",
    "            z_click = []\n",
    "            for ((x_click, _),) in self.v_click:\n",
    "                z_click.append(pl_module.seq_encoder_click(x_click.to(self.device)))\n",
    "            z_click = torch.cat(z_click, dim=0)\n",
    "\n",
    "            T = z_trx.size(0)\n",
    "            C = z_click.size(0)\n",
    "            device = z_trx.device\n",
    "            ix_t = torch.arange(T, device=device).view(-1, 1).expand(T, C).flatten()\n",
    "            ix_c = torch.arange(C, device=device).view(1, -1).expand(T, C).flatten()\n",
    "\n",
    "            z_out = []\n",
    "            for i in range(0, len(ix_t), self.batch_size):\n",
    "                z_pairs = torch.cat([\n",
    "                    z_trx[ix_t[i:i + self.batch_size]],\n",
    "                    z_click[ix_c[i:i + self.batch_size]],\n",
    "                ], dim=1)\n",
    "                z_out.append(pl_module.cls(z_pairs).unsqueeze(1))\n",
    "            z_out = torch.cat(z_out, dim=0).view(T, C)\n",
    "\n",
    "            precision, mrr, r1 = self.logits_to_metrics(z_out)\n",
    "\n",
    "            pl_module.log('valid_full_metrics/precision', precision, prog_bar=True)\n",
    "            pl_module.log('valid_full_metrics/mrr', mrr, prog_bar=False)\n",
    "            pl_module.log('valid_full_metrics/r1', r1, prog_bar=False)\n",
    "\n",
    "        pl_module.to(self.device_main)\n",
    "        if was_traning:\n",
    "            pl_module.train()\n",
    "\n",
    "    def logits_to_metrics(self, z_out):\n",
    "        T, C = z_out.size()\n",
    "        z_ranks = torch.zeros_like(z_out)\n",
    "        z_ranks[\n",
    "            torch.arange(T, device=self.device).view(-1, 1).expand(T, C),\n",
    "            torch.argsort(z_out, dim=1, descending=True),\n",
    "        ] = torch.arange(C, device=self.device).float().view(1, -1).expand(T, C) + 1\n",
    "        z_ranks = torch.cat([\n",
    "            torch.ones(T, device=self.device).float().view(-1, 1),\n",
    "            z_ranks + 1,\n",
    "        ], dim=1)\n",
    "        \n",
    "        click_uids = np.concatenate([['0'], self.v_click.dataset.pairs[:, 0]])\n",
    "        true_ranks = z_ranks[\n",
    "            np.arange(T),\n",
    "            np.searchsorted(click_uids,\n",
    "                            self.target.set_index('bank')['rtk'].loc[self.v_trx.dataset.pairs[:, 0]].values)\n",
    "        ]\n",
    "        precision = torch.where(true_ranks <= self.k,\n",
    "                                torch.ones(1, device=self.device), torch.zeros(1, device=self.device)).mean()\n",
    "        mrr = torch.where(true_ranks <= self.k, 1 / true_ranks, torch.zeros(1, device=self.device)).mean()\n",
    "        r1 = 2 * mrr * precision / (mrr + precision)\n",
    "        return precision, mrr, r1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-stamp",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "twenty-organ",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=[0],\n",
    "    max_steps=3000,\n",
    "    callbacks=[\n",
    "        pl.callbacks.LearningRateMonitor(),\n",
    "        pl.callbacks.ModelCheckpoint(\n",
    "            every_n_train_steps=1000, save_top_k=-1,\n",
    "        ),\n",
    "        ValidationCallback(valid_dl_trx, valid_dl_click, df_matching_valid,\n",
    "                           torch.device('cuda:0'), torch.device('cuda:0')),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-testimony",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name               | Type                   | Params\n",
      "--------------------------------------------------------------\n",
      "0 | rnn_enc            | Sequential             | 395 K \n",
      "1 | _seq_encoder_trx   | Sequential             | 52.0 K\n",
      "2 | _seq_encoder_click | Sequential             | 69.2 K\n",
      "3 | mlm_model_click    | MLMPretrainModuleClick | 4.0 M \n",
      "4 | cls                | Sequential             | 0     \n",
      "5 | train_precision    | PrecisionK             | 0     \n",
      "6 | train_mrr          | MeanReciprocalRankK    | 0     \n",
      "7 | valid_precision    | PrecisionK             | 0     \n",
      "8 | valid_mrr          | MeanReciprocalRankK    | 0     \n",
      "--------------------------------------------------------------\n",
      "2.9 M     Trainable params\n",
      "1.5 M     Non-trainable params\n",
      "4.4 M     Total params\n",
      "17.687    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae0fc1ac66c42ec82b407446634fdef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(sup_model, train_dl)  # valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b61be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a4c60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295ee83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2663d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "0: v78\n",
    "1: v79\n",
    "2: v80\n",
    "3: v81\n",
    "4: v82\n",
    "5: v83\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-evanescence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "supreme-subsection",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_m = get_scalars('lightning_logs/').set_index('version').loc[[f'version_{i}' for i in [78, 79, 80, 81, 82, 83]]]\n",
    "\n",
    "# df = df_m[lambda x: x['tag'].str.startswith('train_metrics')] \\\n",
    "# .pivot(index='step', columns='tag', values='value')\n",
    "# _, axs = plt.subplots(2, 1, figsize=(16, 15))\n",
    "# for col, ax in zip(df.columns, axs):\n",
    "#     df[col].plot(ax=ax, title=col, grid=True)\n",
    "# plt.show()\n",
    "\n",
    "# df = df_m[lambda x: x['tag'].str.startswith('valid_full_metrics')] \\\n",
    "# .pivot(index='step', columns='tag', values='value')\n",
    "# _, axs = plt.subplots(3, 1, figsize=(16, 18))\n",
    "# for col, ax in zip(df.columns, axs):\n",
    "#     df[col].plot(ax=ax, title=col, grid=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "surgical-amateur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>version</th>\n",
       "      <th>version_78</th>\n",
       "      <th>version_79</th>\n",
       "      <th>version_80</th>\n",
       "      <th>version_81</th>\n",
       "      <th>version_82</th>\n",
       "      <th>version_83</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>valid_full_metrics/mrr</th>\n",
       "      <td>0.1972</td>\n",
       "      <td>0.1955</td>\n",
       "      <td>0.1959</td>\n",
       "      <td>0.1936</td>\n",
       "      <td>0.1946</td>\n",
       "      <td>0.1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valid_full_metrics/precision</th>\n",
       "      <td>0.5174</td>\n",
       "      <td>0.5014</td>\n",
       "      <td>0.5038</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>0.4966</td>\n",
       "      <td>0.4971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valid_full_metrics/r1</th>\n",
       "      <td>0.2856</td>\n",
       "      <td>0.2813</td>\n",
       "      <td>0.2821</td>\n",
       "      <td>0.2772</td>\n",
       "      <td>0.2796</td>\n",
       "      <td>0.2808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "version                       version_78  version_79  version_80  version_81  \\\n",
       "tag                                                                            \n",
       "valid_full_metrics/mrr            0.1972      0.1955      0.1959      0.1936   \n",
       "valid_full_metrics/precision      0.5174      0.5014      0.5038      0.4877   \n",
       "valid_full_metrics/r1             0.2856      0.2813      0.2821      0.2772   \n",
       "\n",
       "version                       version_82  version_83  \n",
       "tag                                                   \n",
       "valid_full_metrics/mrr            0.1946      0.1957  \n",
       "valid_full_metrics/precision      0.4966      0.4971  \n",
       "valid_full_metrics/r1             0.2796      0.2808  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m[lambda x: x['tag'].str.startswith('valid_full_metrics')] \\\n",
    ".pivot_table(index='tag', columns='version', values='value', aggfunc=lambda x: x[-1]).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b21b9bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag\n",
       "valid_full_metrics/mrr          0.1954\n",
       "valid_full_metrics/precision    0.5007\n",
       "valid_full_metrics/r1           0.2811\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m[lambda x: x['tag'].str.startswith('valid_full_metrics')] \\\n",
    ".pivot_table(index='tag', columns='version', values='value', aggfunc=lambda x: x[-1]).mean(axis=1).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cfee29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vtb",
   "language": "python",
   "name": "vtb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
