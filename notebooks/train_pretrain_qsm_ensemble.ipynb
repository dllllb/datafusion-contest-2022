{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "simplified-country",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/kireev/pycharm-deploy/vtb\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a60312c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "joined-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "restricted-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d326f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "subtle-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "oriented-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "labeled-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhocon import ConfigFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "outer-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "monthly-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.data_load.iterable_processing.category_size_clip import CategorySizeClip\n",
    "from dltranz.data_load import augmentation_chain\n",
    "from dltranz.data_load.augmentations.seq_len_limit import SeqLenLimit\n",
    "from dltranz.data_load.augmentations.random_slice import RandomSlice\n",
    "\n",
    "from dltranz.seq_encoder import create_encoder\n",
    "\n",
    "from dltranz.metric_learn.sampling_strategies import get_sampling_strategy\n",
    "from dltranz.metric_learn.losses import get_loss\n",
    "\n",
    "from dltranz.tb_interface import get_scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "latter-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vtb_code.data import PairedDataset, paired_collate_fn, PairedZeroDataset, DropDuplicate\n",
    "from vtb_code.metrics import PrecisionK, MeanReciprocalRankK, ValidationCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-administrator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-thong",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD_ID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_id_test = FOLD_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_count = len(glob('data/train_matching_*.csv'))\n",
    "folds_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_id_valid = np.random.choice([i for i in range(folds_count) if i != fold_id_test], size=1)[0]\n",
    "fold_id_valid = (fold_id_test + 1) % folds_count\n",
    "fold_id_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matching_train = pd.concat([pd.read_csv(f'data/train_matching_{i}.csv')\n",
    "                              for i in range(folds_count) \n",
    "                              if i not in (fold_id_test, fold_id_valid)])\n",
    "df_matching_valid = pd.read_csv(f'data/train_matching_{fold_id_valid}.csv')\n",
    "df_matching_test = pd.read_csv(f'data/train_matching_{fold_id_test}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "del fold_id_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(f'data/features_f{FOLD_ID}.pickle', 'rb') as f:\n",
    "    (\n",
    "        features_trx_train,\n",
    "        features_trx_valid,\n",
    "        features_trx_test,\n",
    "        features_trx_puzzle,\n",
    "        features_click_train,\n",
    "        features_click_valid,\n",
    "        features_click_test,\n",
    "        features_click_puzzle,\n",
    "    ) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a652e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da05d450",
   "metadata": {},
   "source": [
    "# Preetrain, one for all models in ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed1be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_mlm_trx = torch.utils.data.DataLoader(\n",
    "    PairedDataset(\n",
    "        np.sort(np.array(list(features_trx_train.keys()))).reshape(-1, 1),\n",
    "        data=[features_trx_train],\n",
    "        augmentations=[augmentation_chain(\n",
    "            DropDuplicate('mcc_code', col_new_cnt='c_cnt'), \n",
    "            RandomSlice(32, 128)\n",
    "        )],\n",
    "        n_sample=1,\n",
    "    ),\n",
    "    collate_fn=paired_collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=12,\n",
    "    batch_size=128,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d69ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_mlm_click = torch.utils.data.DataLoader(\n",
    "    PairedDataset(\n",
    "        np.sort(np.array(list(features_click_train.keys()))).reshape(-1, 1),\n",
    "        data=[features_click_train],\n",
    "        augmentations=[augmentation_chain(\n",
    "            DropDuplicate('cat_id', col_new_cnt='c_cnt'), \n",
    "            RandomSlice(32, 128)\n",
    "        )],\n",
    "        n_sample=1,\n",
    "    ),\n",
    "    collate_fn=paired_collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=12,\n",
    "    batch_size=128,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a177ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = []\n",
    "for batch in train_dl_mlm_trx:\n",
    "    v.append(batch[0][0].payload['transaction_amt'][batch[0][0].seq_len_mask.bool()])\n",
    "v = torch.cat(v)\n",
    "\n",
    "trx_amnt_quantiles = torch.quantile(torch.unique(v), torch.linspace(0, 1, 100))\n",
    "trx_amnt_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f874c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8b98778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.trx_encoder import TrxEncoder, PaddedBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "289c6011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vtb_code.models import MeanLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mechanical-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrxTransform(torch.nn.Module):\n",
    "    def __init__(self, trx_amnt_quantiles):\n",
    "        super().__init__()\n",
    "        self.trx_amnt_quantiles = torch.nn.Parameter(trx_amnt_quantiles, requires_grad=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x.payload['transaction_amt_q'] = torch.bucketize(x.payload['transaction_amt'], self.trx_amnt_quantiles) + 1\n",
    "        return x\n",
    "    \n",
    "class CustomClickTransform(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "#         x.payload['cat_id'] = torch.clamp(x.payload['cat_id'], 0, 300)\n",
    "#         x.payload['level_0'] = torch.clamp(x.payload['level_0'], 0, 200)\n",
    "#         x.payload['level_1'] = torch.clamp(x.payload['level_1'], 0, 200)\n",
    "#         x.payload['level_2'] = torch.clamp(x.payload['level_2'], 0, 200)\n",
    "#         x.payload['c_cnt_clamp'] = torch.clamp(x.payload['c_cnt'], 0, 20).int()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "395037bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateFeaturesTransform(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        et = x.payload['event_time'].int()\n",
    "        et_day = et.div(24 * 60 * 60, rounding_mode='floor').int()\n",
    "        x.payload['hour'] = et.div(60 * 60, rounding_mode='floor') % 24 + 1\n",
    "        x.payload['weekday'] = et.div(60 * 60 * 24, rounding_mode='floor') % 7 + 1\n",
    "        x.payload['day_diff'] = torch.clamp(torch.diff(et_day, prepend=et_day[:, :1], dim=1), 0, 14)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1d9e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBLinear(torch.nn.Linear):\n",
    "    def forward(self, x: PaddedBatch):\n",
    "        return PaddedBatch(super().forward(x.payload), x.seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed9272aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBL2Norm(torch.nn.Module):\n",
    "    def __init__(self, beta):    \n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return PaddedBatch(self.beta * x.payload / (x.payload.pow(2).sum(dim=-1, keepdim=True) + 1e-9).pow(0.5), \n",
    "                           x.seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "952d0fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vtb_code.data import frequency_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6396ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMPretrainModule(pl.LightningModule):\n",
    "    def __init__(self, data_type, params,\n",
    "                 lr, weight_decay,\n",
    "                 max_lr, pct_start, total_steps,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        common_trx_size = params['common_trx_size']\n",
    "        self.seq_encoder = None\n",
    "        \n",
    "        self.token_mask = torch.nn.Parameter(torch.randn(1, 1, common_trx_size), requires_grad=True)\n",
    "        self.transf = torch.nn.TransformerEncoder(\n",
    "            encoder_layer=torch.nn.TransformerEncoderLayer(\n",
    "                d_model=common_trx_size,\n",
    "                nhead=params['transf.nhead'],\n",
    "                dim_feedforward=params['transf.dim_feedforward'],\n",
    "                dropout=params['transf.dropout'],\n",
    "                batch_first=True,\n",
    "            ),\n",
    "            num_layers=params['transf.num_layers'], \n",
    "            norm=torch.nn.LayerNorm(common_trx_size) if params['transf.norm'] else None,\n",
    "        )\n",
    "        \n",
    "        if params['transf.use_pe']:\n",
    "            self.pe = torch.nn.Parameter(self.get_pe(), requires_grad=False)\n",
    "        else:\n",
    "            self.pe = None\n",
    "        self.padding_mask = torch.nn.Parameter(torch.tensor([True, False]).bool(), requires_grad=False)\n",
    "\n",
    "        self.train_mlm_loss_all = MeanLoss(compute_on_step=False)\n",
    "        self.valid_mlm_loss_all = MeanLoss(compute_on_step=False)\n",
    "        self.train_mlm_loss_self = MeanLoss(compute_on_step=False)\n",
    "        self.valid_mlm_loss_self = MeanLoss(compute_on_step=False)\n",
    "        \n",
    "    def get_pe(self):\n",
    "        max_len = self.hparams.params['transf.max_len']\n",
    "        H = self.hparams.params['common_trx_size']\n",
    "        f = 2 * np.pi * torch.arange(max_len).view(1, -1, 1) / \\\n",
    "        torch.exp(torch.linspace(*np.log([4, max_len]), H // 2)).view(1, 1, -1)\n",
    "        return torch.cat([torch.sin(f), torch.cos(f)], dim=2)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer=optim,\n",
    "            max_lr=self.hparams.max_lr,\n",
    "            total_steps=self.hparams.total_steps,\n",
    "            pct_start=self.hparams.pct_start,\n",
    "            anneal_strategy='cos',\n",
    "            cycle_momentum=False,\n",
    "            div_factor=25.0,\n",
    "            final_div_factor=10000.0,\n",
    "            three_phase=True,\n",
    "        )\n",
    "        scheduler = {'scheduler': scheduler, 'interval': 'step'}\n",
    "        return [optim], [scheduler]\n",
    "            \n",
    "    def get_mask(self, x: PaddedBatch):\n",
    "        return torch.bernoulli(x.seq_len_mask.float() * self.hparams.params['mlm.replace_proba']).bool()\n",
    "        \n",
    "    def mask_x(self, x: PaddedBatch, mask):\n",
    "        return torch.where(mask.unsqueeze(2).expand_as(x.payload), \n",
    "                           self.token_mask.expand_as(x.payload), x.payload)\n",
    "        \n",
    "    def get_neg_ix(self, mask, neg_type):\n",
    "        \"\"\"Sample from predicts, where `mask == True`, without self element.\n",
    "        For `neg_type='all'` - sample from predicted tokens from batch\n",
    "        For `neg_type='self'` - sample from predicted tokens from row\n",
    "        \"\"\"\n",
    "        if neg_type == 'all':\n",
    "            mn = mask.float().view(1, -1) - \\\n",
    "                torch.eye(mask.numel(), device=mask.device)[mask.flatten()]\n",
    "            neg_ix = torch.multinomial(mn, self.hparams.params['mlm.neg_count_all'])\n",
    "            b_ix = neg_ix.div(mask.size(1), rounding_mode='trunc')\n",
    "            neg_ix = neg_ix % mask.size(1)\n",
    "            return b_ix, neg_ix\n",
    "        if neg_type == 'self':\n",
    "            mask_ix = mask.nonzero(as_tuple=False)\n",
    "            one_pos = torch.eye(mask.size(1), device=mask.device)[mask_ix[:, 1]]\n",
    "            mn = mask[mask_ix[:, 0]].float() - one_pos\n",
    "            mn = mn + 1e-9 * (1 - one_pos)\n",
    "            neg_ix = torch.multinomial(mn, self.hparams.params['mlm.neg_count_self'], replacement=True)\n",
    "            b_ix = mask_ix[:, 0].view(-1, 1).expand_as(neg_ix)\n",
    "            return b_ix, neg_ix\n",
    "        raise AttributeError(f'Unknown neg_type: {neg_type}')\n",
    "    \n",
    "    def sentence_encoding(self, x: PaddedBatch):\n",
    "        return None\n",
    "        \n",
    "    def mlm_loss(self, x: PaddedBatch, neg_type, x_orig: PaddedBatch):\n",
    "        mask = self.get_mask(x)\n",
    "        masked_x = self.mask_x(x, mask)\n",
    "        B, T, H = masked_x.size()\n",
    "        \n",
    "        if self.pe is not None:\n",
    "            if self.training:\n",
    "                start_pos = np.random.randint(0, self.hparams.params['transf.max_len'] - T, 1)[0]\n",
    "            else:\n",
    "                start_pos = 0\n",
    "            pe = self.pe[:, start_pos:start_pos + T]\n",
    "            masked_x = masked_x + pe\n",
    "            \n",
    "        se = self.sentence_encoding(x_orig)\n",
    "        if se is not None:\n",
    "            masked_x = masked_x + se\n",
    "\n",
    "        out = self.transf(masked_x, src_key_padding_mask=self.padding_mask[x.seq_len_mask])\n",
    "        \n",
    "        if self.pe is not None:\n",
    "            out = out - pe\n",
    "        if se is not None:\n",
    "            out = out - se\n",
    "\n",
    "        target = x.payload[mask].unsqueeze(1)  # N, 1, H\n",
    "        predict = out[mask].unsqueeze(1) # N, 1, H\n",
    "        neg_ix = self.get_neg_ix(mask, neg_type)\n",
    "        negative = out[neg_ix[0], neg_ix[1]]  # N, nneg, H\n",
    "        out_samples = torch.cat([predict, negative], dim=1)\n",
    "        probas = torch.softmax((target * out_samples).sum(dim=2), dim=1)\n",
    "        loss = -torch.log(probas[:, 0])\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x_trx, _), = batch\n",
    "        \n",
    "        z_trx = self.seq_encoder(x_trx)  # PB: B, T, H\n",
    "        \n",
    "        loss_mlm = self.mlm_loss(z_trx, neg_type='all', x_orig=x_trx)\n",
    "        self.train_mlm_loss_all(loss_mlm)\n",
    "        loss_mlm_all = loss_mlm.mean()\n",
    "        self.log(f'loss/mlm_{self.hparams.data_type}', loss_mlm_all)\n",
    "\n",
    "        loss_mlm = self.mlm_loss(z_trx, neg_type='self', x_orig=x_trx)\n",
    "        self.train_mlm_loss_self(loss_mlm)\n",
    "        loss_mlm_self = loss_mlm.mean()\n",
    "        self.log(f'loss/mlm_{self.hparams.data_type}_self', loss_mlm_self)\n",
    "        \n",
    "        return loss_mlm_all + loss_mlm_self\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        (x_trx, _), = batch\n",
    "        z_trx = self.seq_encoder(x_trx)  # PB: B, T, H\n",
    "        \n",
    "        loss_mlm = self.mlm_loss(z_trx, neg_type='all', x_orig=x_trx)\n",
    "        self.valid_mlm_loss_all(loss_mlm)\n",
    "        \n",
    "        loss_mlm = self.mlm_loss(z_trx, neg_type='self', x_orig=x_trx)\n",
    "        self.valid_mlm_loss_self(loss_mlm)\n",
    "        \n",
    "    def training_epoch_end(self, _):\n",
    "        self.log(f'metrics/train_{self.hparams.data_type}_mlm', self.train_mlm_loss_all, prog_bar=False)\n",
    "        self.log(f'metrics/train_{self.hparams.data_type}_mlm_self', self.train_mlm_loss_self, prog_bar=False)\n",
    "        \n",
    "    def validation_epoch_end(self, _):\n",
    "        self.log(f'metrics/valid_{self.hparams.data_type}_mlm', self.valid_mlm_loss_all, prog_bar=True)\n",
    "        self.log(f'metrics/valid_{self.hparams.data_type}_mlm_self', self.valid_mlm_loss_self, prog_bar=True)\n",
    "\n",
    "        \n",
    "class MLMPretrainModuleTrx(MLMPretrainModule):\n",
    "    def __init__(self,\n",
    "                 trx_amnt_quantiles, \n",
    "                 params,\n",
    "                 lr, weight_decay,\n",
    "                 max_lr, pct_start, total_steps,\n",
    "                ):\n",
    "        super().__init__(data_type='trx',\n",
    "                         params=params,\n",
    "                         lr=lr, weight_decay=weight_decay,\n",
    "                         max_lr=max_lr, pct_start=pct_start, total_steps=total_steps,\n",
    "                        )\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        common_trx_size = self.hparams.params['common_trx_size']\n",
    "        t = TrxEncoder(self.hparams.params['trx_seq.trx_encoder'])\n",
    "        self.seq_encoder = torch.nn.Sequential(\n",
    "            CustomTrxTransform(trx_amnt_quantiles=trx_amnt_quantiles),\n",
    "            DateFeaturesTransform(),\n",
    "            t, PBLinear(t.output_size, common_trx_size),\n",
    "            PBL2Norm(self.hparams.params['mlm.beta']),\n",
    "        )\n",
    "        \n",
    "\n",
    "class MLMPretrainModuleClick(MLMPretrainModule):\n",
    "    def __init__(self, params,\n",
    "                 lr, weight_decay,\n",
    "                 max_lr, pct_start, total_steps,\n",
    "                ):\n",
    "        super().__init__(data_type='click',\n",
    "                         params=params,\n",
    "                         lr=lr, weight_decay=weight_decay,\n",
    "                         max_lr=max_lr, pct_start=pct_start, total_steps=total_steps,\n",
    "                        )\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        common_trx_size = self.hparams.params['common_trx_size']\n",
    "        t = TrxEncoder(self.hparams.params['click_seq.trx_encoder'])\n",
    "        self.seq_encoder = torch.nn.Sequential(\n",
    "            CustomClickTransform(),\n",
    "            DateFeaturesTransform(),\n",
    "            t, PBLinear(t.output_size, common_trx_size),\n",
    "            PBL2Norm(self.hparams.params['mlm.beta']),\n",
    "        )\n",
    "        \n",
    "#         self.se = torch.nn.Embedding(7, common_trx_size, padding_idx=0)\n",
    "\n",
    "#     def sentence_encoding(self, x: PaddedBatch):\n",
    "#         se = torch.stack([frequency_encoder(v, m.bool())\n",
    "#                           for v, m in zip(x.payload['new_uid'], x.seq_len_mask)], dim=0).clamp(None, 6)\n",
    "#         return self.se(se)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a087409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigFactory.parse_string('''\n",
    "    common_trx_size: 256\n",
    "    transf: {\n",
    "        nhead: 4\n",
    "        dim_feedforward: 1024\n",
    "        dropout: 0.1\n",
    "        num_layers: 3\n",
    "        norm: false\n",
    "        max_len: 6000\n",
    "        use_pe: true\n",
    "    }\n",
    "    mlm: {\n",
    "        replace_proba: 0.1\n",
    "        neg_count_all: 64\n",
    "        neg_count_self: 8\n",
    "        beta: 10\n",
    "    }\n",
    "    trx_seq: {\n",
    "        trx_encoder: {\n",
    "          use_batch_norm_with_lens: false\n",
    "          norm_embeddings: false,\n",
    "          embeddings_noise: 0.000,\n",
    "          embeddings: {\n",
    "            mcc_code: {in: 350, out: 64},\n",
    "            currency_rk: {in: 10, out: 4}\n",
    "            transaction_amt_q: {in: 110, out: 8}\n",
    "            \n",
    "            hour: {in: 30, out: 16}\n",
    "            weekday: {in: 10, out: 4}\n",
    "            day_diff: {in: 15, out: 8}\n",
    "          },\n",
    "          numeric_values: {\n",
    "            transaction_amt: identity\n",
    "            c_cnt: log\n",
    "          }\n",
    "          was_logified: false\n",
    "          log_scale_factor: 1.0\n",
    "        },\n",
    "    }\n",
    "    click_seq: {\n",
    "        trx_encoder: {\n",
    "          use_batch_norm_with_lens: false\n",
    "          norm_embeddings: false,\n",
    "          embeddings_noise: 0.000,\n",
    "          embeddings: {\n",
    "            cat_id: {in: 400, out: 64},\n",
    "            level_0: {in: 400, out: 16}\n",
    "            level_1: {in: 400, out: 8}\n",
    "            level_2: {in: 400, out: 4}\n",
    "            \n",
    "            hour: {in: 30, out: 16}\n",
    "            weekday: {in: 10, out: 4}\n",
    "            day_diff: {in: 15, out: 8}\n",
    "          },\n",
    "          numeric_values: {\n",
    "            c_cnt: log\n",
    "          }\n",
    "          was_logified: false\n",
    "          log_scale_factor: 1.0\n",
    "        },\n",
    "    }\n",
    "''')\n",
    "\n",
    "mlm_model_trx = MLMPretrainModuleTrx(\n",
    "    params=config,                     \n",
    "    lr=0.001, weight_decay=0,\n",
    "    max_lr=0.001, pct_start=9000 / 2 / 10000, total_steps=10000,\n",
    "    trx_amnt_quantiles=trx_amnt_quantiles,\n",
    ")\n",
    "mlm_model_click = MLMPretrainModuleClick(\n",
    "    params=config,                     \n",
    "    lr=0.001, weight_decay=0,\n",
    "    max_lr=0.001, pct_start=9000 / 2 / 10000, total_steps=10000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcde695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b702285",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=[0],\n",
    "    max_steps=8000,\n",
    "    enable_progress_bar=False,\n",
    "    callbacks=[\n",
    "        pl.callbacks.LearningRateMonitor(),\n",
    "        pl.callbacks.ModelCheckpoint(\n",
    "            every_n_train_steps=2000, save_top_k=-1,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "model_version_trx = trainer.logger.version\n",
    "print('baseline all:  {:.3f}'.format(np.log(mlm_model_trx.hparams.params['mlm.neg_count_all'] + 1)))\n",
    "print('baseline self: {:.3f}'.format(np.log(mlm_model_trx.hparams.params['mlm.neg_count_self'] + 1)))\n",
    "print(f'version = {model_version_trx}')\n",
    "trainer.fit(mlm_model_trx, train_dl_mlm_trx)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b739381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=[0],\n",
    "    max_steps=6000,\n",
    "    enable_progress_bar=False,\n",
    "    callbacks=[\n",
    "        pl.callbacks.LearningRateMonitor(),\n",
    "        pl.callbacks.ModelCheckpoint(\n",
    "            every_n_train_steps=2000, save_top_k=-1,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "model_version_click = trainer.logger.version\n",
    "print('baseline all:  {:.3f}'.format(np.log(mlm_model_click.hparams.params['mlm.neg_count_all'] + 1)))\n",
    "print('baseline self: {:.3f}'.format(np.log(mlm_model_click.hparams.params['mlm.neg_count_self'] + 1)))\n",
    "print(f'version = {model_version_click}')\n",
    "trainer.fit(mlm_model_click, train_dl_mlm_click)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146538b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82bfc3f4",
   "metadata": {},
   "source": [
    "# Use pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "pursuant-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.seq_encoder.utils import NormEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c992ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.trx_encoder import TrxEncoder, PaddedBatch\n",
    "from dltranz.seq_encoder.rnn_encoder import RnnEncoder\n",
    "from dltranz.seq_encoder.utils import LastStepEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "crude-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2Scorer(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        B, H = x.size()\n",
    "        a, b =x[:, :H // 2], x[:, H // 2:]\n",
    "        return -(a - b).pow(2).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2698f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBLayerNorm(torch.nn.LayerNorm):\n",
    "    def forward(self, x: PaddedBatch):\n",
    "        return PaddedBatch(super().forward(x.payload), x.seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "lesbian-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedModule(pl.LightningModule):\n",
    "    def __init__(self, params, k,\n",
    "                 lr, weight_decay,\n",
    "                 max_lr, pct_start, total_steps,\n",
    "                 beta, neg_count,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        common_trx_size = mlm_model_trx.hparams.params['common_trx_size']\n",
    "        self.rnn_enc =  torch.nn.Sequential(\n",
    "            RnnEncoder(common_trx_size, params['rnn']), \n",
    "            LastStepEncoder(),\n",
    "#             NormEncoder(),\n",
    "        )\n",
    "        self._seq_encoder_trx = torch.nn.Sequential(\n",
    "            mlm_model_trx.seq_encoder,\n",
    "            PBLayerNorm(common_trx_size),\n",
    "        )\n",
    "        self._seq_encoder_click = torch.nn.Sequential(\n",
    "            mlm_model_click.seq_encoder,\n",
    "            PBLayerNorm(common_trx_size),\n",
    "        )\n",
    "        self.mlm_model_click = mlm_model_click\n",
    "        \n",
    "        self.cls = torch.nn.Sequential(\n",
    "            L2Scorer(),\n",
    "        )\n",
    "\n",
    "        self.train_precision = PrecisionK(k=k, compute_on_step=False)\n",
    "        self.train_mrr = MeanReciprocalRankK(k=k, compute_on_step=False)\n",
    "        self.valid_precision = PrecisionK(k=k, compute_on_step=False)\n",
    "        self.valid_mrr = MeanReciprocalRankK(k=k, compute_on_step=False)\n",
    "        \n",
    "    def seq_encoder_trx(self, x):\n",
    "        x = self._seq_encoder_trx(x)\n",
    "        return self.rnn_enc(x)\n",
    "    \n",
    "    def seq_encoder_click(self, x_orig):\n",
    "        x = self._seq_encoder_click(x_orig)\n",
    "#         x = PaddedBatch(\n",
    "#             x.payload + self.mlm_model_click.sentence_encoding(x_orig),\n",
    "#             x.seq_lens,\n",
    "#         )\n",
    "        return self.rnn_enc(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer=optim,\n",
    "            max_lr=self.hparams.max_lr,\n",
    "            total_steps=self.hparams.total_steps,\n",
    "            pct_start=self.hparams.pct_start,\n",
    "            anneal_strategy='cos',\n",
    "            cycle_momentum=False,\n",
    "            div_factor=25.0,\n",
    "            final_div_factor=10000.0,\n",
    "            three_phase=True,\n",
    "        )\n",
    "        scheduler = {'scheduler': scheduler, 'interval': 'step'}\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "    def loss_fn_p(self, embeddings, labels, ref_emb, ref_labels):\n",
    "        beta = self.hparams.beta\n",
    "        neg_count = self.hparams.neg_count\n",
    "        \n",
    "        pos_ix = (labels.view(-1, 1) == ref_labels.view(1, -1)).nonzero(as_tuple=False)\n",
    "        pos_labels = labels[pos_ix[:, 0]]\n",
    "        neg_w = ((pos_labels.view(-1, 1) != ref_labels.view(1, -1))).float()\n",
    "        neg_ix = torch.multinomial(neg_w, neg_count - 1)\n",
    "        all_ix = torch.cat([pos_ix[:, [1]], neg_ix], dim=1)\n",
    "        logits = -(embeddings[pos_ix[:, [0]]] - ref_emb[all_ix]).pow(2).sum(dim=2)\n",
    "        logits = logits * beta\n",
    "        logs = -torch.log(torch.softmax(logits, dim=1))[:, 0]\n",
    "#         logs = torch.relu(logs + np.log(0.1))\n",
    "        return logs.mean()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # pairs\n",
    "        x_trx, l_trx, m_trx, x_click, l_click, m_click = batch\n",
    "        z_trx = self.seq_encoder_trx(x_trx)  # B, H\n",
    "        z_click = self.seq_encoder_click(x_click)  # B, H\n",
    "        loss_pt = self.loss_fn_p(embeddings=z_trx, labels=l_trx, ref_emb=z_click, ref_labels=l_click)\n",
    "        self.log('loss/loss_pt', loss_pt)\n",
    "        \n",
    "        loss_pc = self.loss_fn_p(embeddings=z_click, labels=l_click, ref_emb=z_trx, ref_labels=l_trx)\n",
    "        self.log('loss/loss_pc', loss_pc)\n",
    "       \n",
    "        with torch.no_grad():\n",
    "            out = -(z_trx.unsqueeze(1) - z_click.unsqueeze(0)).pow(2).sum(dim=2)\n",
    "            out = out[m_trx == 0][:, m_click == 0]\n",
    "            T, C = out.size()\n",
    "            assert T == C\n",
    "            n_samples = z_trx.size(0) // (l_trx.max().item() + 1)\n",
    "            for i in range(n_samples):\n",
    "                l2 = out[i::n_samples, i::n_samples]\n",
    "                self.train_precision(l2)\n",
    "                self.train_mrr(l2)\n",
    "        \n",
    "        return loss_pt + 0.1 * loss_pc  #  loss_pc \n",
    "\n",
    "    def training_epoch_end(self, _):\n",
    "        self.log('train_metrics/precision', self.train_precision, prog_bar=True)\n",
    "        self.log('train_metrics/mrr', self.train_mrr, prog_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-corpus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "for enseble_fold_id in range(folds_count):\n",
    "    if enseble_fold_id == fold_id_valid:\n",
    "        continue\n",
    "    \n",
    "    batch_size = 128\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        PairedZeroDataset(\n",
    "            pd.concat([df_matching_train, df_matching_test], axis=0)[lambda x: x['rtk'].ne('0')].values,\n",
    "            data=[\n",
    "                dict(chain(features_trx_train.items(), features_trx_test.items())),\n",
    "                dict(chain(features_click_train.items(), features_click_test.items())),\n",
    "            ],\n",
    "            augmentations=[\n",
    "                augmentation_chain(DropDuplicate('mcc_code', col_new_cnt='c_cnt'), RandomSlice(32, 1024)),  # 1024\n",
    "                augmentation_chain(DropDuplicate('cat_id', col_new_cnt='c_cnt'), RandomSlice(64, 2048)),  # 2048\n",
    "            ],\n",
    "            n_sample=2,\n",
    "        ),\n",
    "        collate_fn=PairedZeroDataset.collate_fn,\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        num_workers=24,\n",
    "        batch_size=batch_size,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    \n",
    "#     mlm_model_trx = MLMPretrainModuleTrx.load_from_checkpoint(\n",
    "#         'lightning_logs/version_/checkpoints/epoch=86-step=7999.ckpt')  # 42\n",
    "#     mlm_model_click = MLMPretrainModuleClick.load_from_checkpoint(\n",
    "#         'lightning_logs/version_/checkpoints/epoch=77-step=5999.ckpt')  # 43\n",
    "    pl.seed_everything(random.randint(1, 2**16-1))\n",
    "    \n",
    "    sup_model = PairedModule(\n",
    "        ConfigFactory.parse_string('''\n",
    "        common_trx_size: 128\n",
    "        rnn: {\n",
    "          type: gru,\n",
    "          hidden_size: 256,\n",
    "          bidir: false,\n",
    "          trainable_starter: static\n",
    "        }\n",
    "    '''),                     \n",
    "        k=100 * batch_size // 3000,\n",
    "        lr=0.0022, weight_decay=0,\n",
    "        max_lr=0.0018, pct_start=1100 / 6000, total_steps=6000,\n",
    "        beta=0.2 / 1.4, neg_count=120,\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        gpus[0],\n",
    "        max_steps=3000,\n",
    "        enable_progress_bar=False,\n",
    "        enable_model_summary=False,\n",
    "        callbacks=[\n",
    "            pl.callbacks.LearningRateMonitor(),\n",
    "            pl.callbacks.ModelCheckpoint(\n",
    "                every_n_train_steps=1000, save_top_k=-1,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    print(f'FOLD_ID={FOLD_ID}, fold_id_valid={fold_id_valid}, '\n",
    "          f'enseble_fold_id={enseble_fold_id}, version={trainer.logger.version}')\n",
    "    trainer.fit(sup_model, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-arena",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-boating",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b61be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "moral-newark",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecological-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationCallback():\n",
    "    def __init__(self, v_trx, v_click, target, device, k=100, batch_size=1024):\n",
    "        self.v_trx = v_trx\n",
    "        self.v_click = v_click\n",
    "        self.target = target\n",
    "        self.k = k\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "    def run(self, model):\n",
    "        device = self.device\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z_trx = []\n",
    "            for ((x_trx, _),) in self.v_trx:\n",
    "                z_trx.append(model.seq_encoder_trx(x_trx.to(device)))\n",
    "            z_trx = torch.cat(z_trx, dim=0)\n",
    "            z_click = []\n",
    "            for ((x_click, _),) in self.v_click:\n",
    "                z_click.append(model.seq_encoder_click(x_click.to(device)))\n",
    "            z_click = torch.cat(z_click, dim=0)\n",
    "\n",
    "            z_out = []\n",
    "            for i in range(0, z_trx.size(0), self.batch_size):\n",
    "                z_out.append(\n",
    "                    -((z_trx[i:i + self.batch_size].unsqueeze(1) - z_click.unsqueeze(0)).pow(2)).sum(dim=2)\n",
    "                )\n",
    "            z_out = torch.cat(z_out, dim=0)\n",
    "\n",
    "        return z_out\n",
    "        precision, mrr, r1 = self.logits_to_metrics(z_out)\n",
    "\n",
    "        return precision.item(), mrr.item(), r1.item()\n",
    "\n",
    "    def ranks(self, z_out):\n",
    "        T, C = z_out.size()\n",
    "        z_ranks = torch.zeros_like(z_out)\n",
    "        z_ranks[\n",
    "            torch.arange(T, device=z_out.device).view(-1, 1).expand(T, C),\n",
    "            torch.argsort(z_out, dim=1, descending=True),\n",
    "        ] = torch.arange(C, device=z_out.device).float().view(1, -1).expand(T, C) + 1\n",
    "        return z_ranks\n",
    "        \n",
    "    def logits_to_metrics(self, z_out):\n",
    "        T, C = z_out.size()\n",
    "        z_ranks = self.ranks(z_out)\n",
    "        z_ranks = torch.cat([\n",
    "            torch.ones(T, device=z_out.device).float().view(-1, 1),\n",
    "            z_ranks + 1,\n",
    "        ], dim=1)\n",
    "        \n",
    "        click_uids = np.concatenate([['0'], self.v_click.dataset.pairs[:, 0]])\n",
    "        true_ranks = z_ranks[\n",
    "            np.arange(T),\n",
    "            np.searchsorted(click_uids,\n",
    "                            self.target.set_index('bank')['rtk'].loc[self.v_trx.dataset.pairs[:, 0]].values)\n",
    "        ]\n",
    "        precision = torch.where(true_ranks <= self.k,\n",
    "                                torch.ones(1, device=z_out.device), torch.zeros(1, device=z_out.device)).mean()\n",
    "        mrr = torch.where(true_ranks <= self.k, 1 / true_ranks, torch.zeros(1, device=z_out.device)).mean()\n",
    "        r1 = 2 * mrr * precision / (mrr + precision)\n",
    "        return precision.item(), mrr.item(), r1.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-central",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "refined-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only model size required here\n",
    "mlm_model_trx = MLMPretrainModuleTrx.load_from_checkpoint(\n",
    "    'lightning_logs/version_0/checkpoints/epoch=21-step=1999.ckpt')  # 42\n",
    "mlm_model_click = MLMPretrainModuleClick.load_from_checkpoint(\n",
    "    'lightning_logs/version_5/checkpoints/epoch=25-step=1999.ckpt')  # 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "moderate-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemple with train_dl.pd.sample(frac=0.85)\n",
    "model_map = [\n",
    "    {'FOLD_ID': 0, 'fold_id_valid': 1, 'mv': [23, 27, 30, 33, 36]},\n",
    "    {'FOLD_ID': 1, 'fold_id_valid': 2, 'mv': [24, 28, 31, 34, 37]},\n",
    "    {'FOLD_ID': 2, 'fold_id_valid': 3, 'mv': [25, 26, 29, 32, 35]},\n",
    "#     {'FOLD_ID': 3, 'fold_id_valid': 4, 'mv': []},\n",
    "#     {'FOLD_ID': 4, 'fold_id_valid': 5, 'mv': []},\n",
    "#     {'FOLD_ID': 5, 'fold_id_valid': 0, 'mv': []},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "coastal-margin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 64104\r\n",
      "-rw-r--r-- 1 ivan sudo 21880483 Apr 21 05:42 'epoch=15-step=1999.ckpt'\r\n",
      "-rw-r--r-- 1 ivan sudo 21880483 Apr 21 05:53 'epoch=23-step=2999.ckpt'\r\n",
      "-rw-r--r-- 1 ivan sudo 21880483 Apr 21 05:31 'epoch=7-step=999.ckpt'\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l lightning_logs/version_23/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "gorgeous-confirmation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD_ID=0, fold_id_valid=1, model_version=23: precision = 0.472, mrr = 0.190, r1 = 0.271\n",
      "FOLD_ID=0, fold_id_valid=1, model_version=27: precision = 0.479, mrr = 0.191, r1 = 0.273\n",
      "FOLD_ID=0, fold_id_valid=1, model_version=30: precision = 0.487, mrr = 0.190, r1 = 0.273\n",
      "FOLD_ID=0, fold_id_valid=1, model_version=33: precision = 0.484, mrr = 0.192, r1 = 0.275\n",
      "FOLD_ID=0, fold_id_valid=1, model_version=36: precision = 0.490, mrr = 0.190, r1 = 0.274\n",
      "FOLD_ID=0, fold_id_valid=1, ensemble distance sum: precision = 0.524, mrr = 0.197, r1 = 0.287\n",
      "FOLD_ID=0, fold_id_valid=1, ensemble distance min: precision = 0.502, mrr = 0.194, r1 = 0.280\n",
      "FOLD_ID=0, fold_id_valid=1, ensemble     rank sum: precision = 0.522, mrr = 0.197, r1 = 0.286\n",
      "FOLD_ID=1, fold_id_valid=2, model_version=24: precision = 0.481, mrr = 0.194, r1 = 0.276\n",
      "FOLD_ID=1, fold_id_valid=2, model_version=28: precision = 0.481, mrr = 0.193, r1 = 0.275\n",
      "FOLD_ID=1, fold_id_valid=2, model_version=31: precision = 0.483, mrr = 0.192, r1 = 0.275\n",
      "FOLD_ID=1, fold_id_valid=2, model_version=34: precision = 0.470, mrr = 0.192, r1 = 0.273\n",
      "FOLD_ID=1, fold_id_valid=2, model_version=37: precision = 0.482, mrr = 0.192, r1 = 0.275\n",
      "FOLD_ID=1, fold_id_valid=2, ensemble distance sum: precision = 0.511, mrr = 0.200, r1 = 0.288\n",
      "FOLD_ID=1, fold_id_valid=2, ensemble distance min: precision = 0.498, mrr = 0.196, r1 = 0.281\n",
      "FOLD_ID=1, fold_id_valid=2, ensemble     rank sum: precision = 0.511, mrr = 0.199, r1 = 0.286\n",
      "FOLD_ID=2, fold_id_valid=3, model_version=25: precision = 0.487, mrr = 0.192, r1 = 0.276\n",
      "FOLD_ID=2, fold_id_valid=3, model_version=26: precision = 0.490, mrr = 0.191, r1 = 0.275\n",
      "FOLD_ID=2, fold_id_valid=3, model_version=29: precision = 0.477, mrr = 0.191, r1 = 0.273\n",
      "FOLD_ID=2, fold_id_valid=3, model_version=32: precision = 0.492, mrr = 0.193, r1 = 0.277\n",
      "FOLD_ID=2, fold_id_valid=3, model_version=35: precision = 0.480, mrr = 0.190, r1 = 0.272\n",
      "FOLD_ID=2, fold_id_valid=3, ensemble distance sum: precision = 0.513, mrr = 0.199, r1 = 0.287\n",
      "FOLD_ID=2, fold_id_valid=3, ensemble distance min: precision = 0.503, mrr = 0.195, r1 = 0.281\n",
      "FOLD_ID=2, fold_id_valid=3, ensemble     rank sum: precision = 0.513, mrr = 0.199, r1 = 0.287\n"
     ]
    }
   ],
   "source": [
    "for mmap in model_map:\n",
    "    FOLD_ID = mmap['FOLD_ID']\n",
    "    fold_id_test = FOLD_ID\n",
    "    folds_count = len(glob('data/train_matching_*.csv'))\n",
    "    fold_id_valid = (fold_id_test + 1) % folds_count\n",
    "    del fold_id_test\n",
    "    df_matching_valid = pd.read_csv(f'data/train_matching_{fold_id_valid}.csv')\n",
    "    with open(f'data/features_f{FOLD_ID}.pickle', 'rb') as f:\n",
    "        (\n",
    "            _,\n",
    "            features_trx_valid,\n",
    "            _,\n",
    "            _,\n",
    "            _,\n",
    "            features_click_valid,\n",
    "            _,\n",
    "            _,\n",
    "        ) = pickle.load(f)\n",
    "    \n",
    "    valid_dl_trx = torch.utils.data.DataLoader(\n",
    "        PairedDataset(\n",
    "            np.sort(df_matching_valid['bank'].unique()).reshape(-1, 1), \n",
    "            data=[\n",
    "                features_trx_valid,\n",
    "            ],\n",
    "            augmentations=[\n",
    "                augmentation_chain(DropDuplicate('mcc_code', col_new_cnt='c_cnt'), SeqLenLimit(2000)),  # 2000\n",
    "            ],\n",
    "            n_sample=1,\n",
    "        ),\n",
    "        collate_fn=paired_collate_fn,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        batch_size=128,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    valid_dl_click = torch.utils.data.DataLoader(\n",
    "        PairedDataset(\n",
    "            np.sort(df_matching_valid[lambda x: x['rtk'].ne('0')]['rtk'].unique()).reshape(-1, 1),\n",
    "            data=[\n",
    "                features_click_valid,\n",
    "            ],\n",
    "            augmentations=[\n",
    "                augmentation_chain(DropDuplicate('cat_id', col_new_cnt='c_cnt'), SeqLenLimit(5000)),  # 5000\n",
    "            ],\n",
    "            n_sample=1,\n",
    "        ),\n",
    "        collate_fn=paired_collate_fn,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        batch_size=128,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    \n",
    "    vc = ValidationCallback(valid_dl_trx, valid_dl_click, df_matching_valid, \n",
    "                            torch.device('cuda:1'))    \n",
    "    res = []\n",
    "    for model_version in mmap['mv']:\n",
    "        sup_model = PairedModule.load_from_checkpoint(\n",
    "            f'lightning_logs/version_{model_version}/checkpoints/epoch=23-step=2999.ckpt')\n",
    "        \n",
    "        zo = vc.run(sup_model)\n",
    "        precision, mrr, r1 = vc.logits_to_metrics(zo)\n",
    "        res.append(zo)\n",
    "        print(f'FOLD_ID={FOLD_ID}, fold_id_valid={mmap[\"fold_id_valid\"]}, model_version={model_version}: '\n",
    "              f'precision = {precision:.3f}, mrr = {mrr:.3f}, r1 = {r1:.3f}')\n",
    "        \n",
    "    precision, mrr, r1 = vc.logits_to_metrics(torch.stack(res, dim=0).sum(dim=0))\n",
    "    print(f'FOLD_ID={FOLD_ID}, fold_id_valid={mmap[\"fold_id_valid\"]}, ensemble distance sum: '\n",
    "          f'precision = {precision:.3f}, mrr = {mrr:.3f}, r1 = {r1:.3f}')\n",
    "    precision, mrr, r1 = vc.logits_to_metrics(torch.stack(res, dim=0).min(dim=0).values)\n",
    "    print(f'FOLD_ID={FOLD_ID}, fold_id_valid={mmap[\"fold_id_valid\"]}, ensemble distance min: '\n",
    "          f'precision = {precision:.3f}, mrr = {mrr:.3f}, r1 = {r1:.3f}')\n",
    "    precision, mrr, r1 = vc.logits_to_metrics(torch.stack([-vc.ranks(o) for o in res], dim=0).sum(dim=0))\n",
    "    print(f'FOLD_ID={FOLD_ID}, fold_id_valid={mmap[\"fold_id_valid\"]}, ensemble     rank sum: '\n",
    "          f'precision = {precision:.3f}, mrr = {mrr:.3f}, r1 = {r1:.3f}')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "false-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemple full data and changing seed\n",
    "model_map = [\n",
    "    {'FOLD_ID': 0, 'fold_id_valid': 1, 'mv': [8, 11, 14, 17, 20]},\n",
    "    {'FOLD_ID': 1, 'fold_id_valid': 2, 'mv': [9, 13, 15, 18, 21]},\n",
    "    {'FOLD_ID': 2, 'fold_id_valid': 3, 'mv': [10, 12, 16, 19, 22]},\n",
    "#     {'FOLD_ID': 3, 'fold_id_valid': 4, 'mv': []},\n",
    "#     {'FOLD_ID': 4, 'fold_id_valid': 5, 'mv': []},\n",
    "#     {'FOLD_ID': 5, 'fold_id_valid': 0, 'mv': []},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "third-member",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD_ID=0, fold_id_valid=1, model_version=8: precision = 0.494, mrr = 0.193, r1 = 0.277\n",
      "FOLD_ID=0, fold_id_valid=1, model_version=11: precision = 0.507, mrr = 0.194, r1 = 0.280\n",
      "FOLD_ID=0, fold_id_valid=1, model_version=14: precision = 0.512, mrr = 0.196, r1 = 0.283\n",
      "FOLD_ID=0, fold_id_valid=1, model_version=17: precision = 0.500, mrr = 0.194, r1 = 0.280\n",
      "FOLD_ID=0, fold_id_valid=1, model_version=20: precision = 0.498, mrr = 0.194, r1 = 0.279\n",
      "FOLD_ID=0, fold_id_valid=1, ensemble distance sum: precision = 0.544, mrr = 0.201, r1 = 0.294\n",
      "FOLD_ID=0, fold_id_valid=1, ensemble distance min: precision = 0.533, mrr = 0.198, r1 = 0.289\n",
      "FOLD_ID=0, fold_id_valid=1, ensemble     rank sum: precision = 0.541, mrr = 0.201, r1 = 0.293\n",
      "FOLD_ID=1, fold_id_valid=2, model_version=9: precision = 0.500, mrr = 0.196, r1 = 0.282\n",
      "FOLD_ID=1, fold_id_valid=2, model_version=13: precision = 0.512, mrr = 0.195, r1 = 0.282\n",
      "FOLD_ID=1, fold_id_valid=2, model_version=15: precision = 0.502, mrr = 0.195, r1 = 0.281\n",
      "FOLD_ID=1, fold_id_valid=2, model_version=18: precision = 0.506, mrr = 0.196, r1 = 0.283\n",
      "FOLD_ID=1, fold_id_valid=2, model_version=21: precision = 0.504, mrr = 0.196, r1 = 0.282\n",
      "FOLD_ID=1, fold_id_valid=2, ensemble distance sum: precision = 0.540, mrr = 0.203, r1 = 0.295\n",
      "FOLD_ID=1, fold_id_valid=2, ensemble distance min: precision = 0.519, mrr = 0.200, r1 = 0.288\n",
      "FOLD_ID=1, fold_id_valid=2, ensemble     rank sum: precision = 0.536, mrr = 0.203, r1 = 0.294\n",
      "FOLD_ID=2, fold_id_valid=3, model_version=10: precision = 0.496, mrr = 0.195, r1 = 0.280\n",
      "FOLD_ID=2, fold_id_valid=3, model_version=12: precision = 0.504, mrr = 0.195, r1 = 0.281\n",
      "FOLD_ID=2, fold_id_valid=3, model_version=16: precision = 0.499, mrr = 0.194, r1 = 0.279\n",
      "FOLD_ID=2, fold_id_valid=3, model_version=19: precision = 0.501, mrr = 0.194, r1 = 0.280\n",
      "FOLD_ID=2, fold_id_valid=3, model_version=22: precision = 0.507, mrr = 0.195, r1 = 0.281\n",
      "FOLD_ID=2, fold_id_valid=3, ensemble distance sum: precision = 0.535, mrr = 0.202, r1 = 0.293\n",
      "FOLD_ID=2, fold_id_valid=3, ensemble distance min: precision = 0.525, mrr = 0.200, r1 = 0.289\n",
      "FOLD_ID=2, fold_id_valid=3, ensemble     rank sum: precision = 0.539, mrr = 0.201, r1 = 0.292\n"
     ]
    }
   ],
   "source": [
    "for mmap in model_map:\n",
    "    FOLD_ID = mmap['FOLD_ID']\n",
    "    fold_id_test = FOLD_ID\n",
    "    folds_count = len(glob('data/train_matching_*.csv'))\n",
    "    fold_id_valid = (fold_id_test + 1) % folds_count\n",
    "    del fold_id_test\n",
    "    df_matching_valid = pd.read_csv(f'data/train_matching_{fold_id_valid}.csv')\n",
    "    with open(f'data/features_f{FOLD_ID}.pickle', 'rb') as f:\n",
    "        (\n",
    "            _,\n",
    "            features_trx_valid,\n",
    "            _,\n",
    "            _,\n",
    "            _,\n",
    "            features_click_valid,\n",
    "            _,\n",
    "            _,\n",
    "        ) = pickle.load(f)\n",
    "    \n",
    "    valid_dl_trx = torch.utils.data.DataLoader(\n",
    "        PairedDataset(\n",
    "            np.sort(df_matching_valid['bank'].unique()).reshape(-1, 1), \n",
    "            data=[\n",
    "                features_trx_valid,\n",
    "            ],\n",
    "            augmentations=[\n",
    "                augmentation_chain(DropDuplicate('mcc_code', col_new_cnt='c_cnt'), SeqLenLimit(2000)),  # 2000\n",
    "            ],\n",
    "            n_sample=1,\n",
    "        ),\n",
    "        collate_fn=paired_collate_fn,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        batch_size=32,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    valid_dl_click = torch.utils.data.DataLoader(\n",
    "        PairedDataset(\n",
    "            np.sort(df_matching_valid[lambda x: x['rtk'].ne('0')]['rtk'].unique()).reshape(-1, 1),\n",
    "            data=[\n",
    "                features_click_valid,\n",
    "            ],\n",
    "            augmentations=[\n",
    "                augmentation_chain(DropDuplicate('cat_id', col_new_cnt='c_cnt'), SeqLenLimit(5000)),  # 5000\n",
    "            ],\n",
    "            n_sample=1,\n",
    "        ),\n",
    "        collate_fn=paired_collate_fn,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        batch_size=32,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    \n",
    "    vc = ValidationCallback(valid_dl_trx, valid_dl_click, df_matching_valid, \n",
    "                            torch.device('cuda:1'))\n",
    "    \n",
    "    res = []\n",
    "    for model_version in mmap['mv']:\n",
    "        sup_model = PairedModule.load_from_checkpoint(\n",
    "            f'lightning_logs/version_{model_version}/checkpoints/epoch=26-step=2999.ckpt')\n",
    "        \n",
    "        zo = vc.run(sup_model)\n",
    "        precision, mrr, r1 = vc.logits_to_metrics(zo)\n",
    "        res.append(zo)\n",
    "        print(f'FOLD_ID={FOLD_ID}, fold_id_valid={mmap[\"fold_id_valid\"]}, model_version={model_version}: '\n",
    "              f'precision = {precision:.3f}, mrr = {mrr:.3f}, r1 = {r1:.3f}')\n",
    "    \n",
    "    precision, mrr, r1 = vc.logits_to_metrics(torch.stack(res, dim=0).sum(dim=0))\n",
    "    print(f'FOLD_ID={FOLD_ID}, fold_id_valid={mmap[\"fold_id_valid\"]}, ensemble distance sum: '\n",
    "          f'precision = {precision:.3f}, mrr = {mrr:.3f}, r1 = {r1:.3f}')\n",
    "    precision, mrr, r1 = vc.logits_to_metrics(torch.stack(res, dim=0).min(dim=0).values)\n",
    "    print(f'FOLD_ID={FOLD_ID}, fold_id_valid={mmap[\"fold_id_valid\"]}, ensemble distance min: '\n",
    "          f'precision = {precision:.3f}, mrr = {mrr:.3f}, r1 = {r1:.3f}')  \n",
    "    precision, mrr, r1 = vc.logits_to_metrics(torch.stack([-vc.ranks(o) for o in res], dim=0).sum(dim=0))\n",
    "    print(f'FOLD_ID={FOLD_ID}, fold_id_valid={mmap[\"fold_id_valid\"]}, ensemble     rank sum: '\n",
    "          f'precision = {precision:.3f}, mrr = {mrr:.3f}, r1 = {r1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-booking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "relevant-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemple full data and changing seed\n",
    "model_map = [\n",
    "    {'FOLD_ID': 0, 'fold_id_valid': 1, 'mv': [42]},\n",
    "    {'FOLD_ID': 1, 'fold_id_valid': 2, 'mv': [41]},\n",
    "    {'FOLD_ID': 2, 'fold_id_valid': 3, 'mv': [43]},\n",
    "#     {'FOLD_ID': 3, 'fold_id_valid': 4, 'mv': []},\n",
    "#     {'FOLD_ID': 4, 'fold_id_valid': 5, 'mv': []},\n",
    "#     {'FOLD_ID': 5, 'fold_id_valid': 0, 'mv': []},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "absent-commodity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 320520\r\n",
      "-rw-r--r-- 1 ivan sudo 21880483 Apr 21 08:48 'epoch=1-step=199.ckpt'\r\n",
      "-rw-r--r-- 1 ivan sudo 21880483 Apr 21 09:03 'epoch=10-step=1199.ckpt'\r\n",
      "-rw-r--r-- 1 ivan sudo 21880483 Apr 21 09:05 'epoch=12-step=1399.ckpt'\r\n",
      "-rw-r--r-- 1 ivan sudo 21880483 Apr 21 09:08 'epoch=14-step=1599.ckpt'\r\n",
      "-rw-r--r-- 1 ivan sudo 21880483 Apr 21 09:10 'epoch=15-step=1799.ckpt'\r\n",
      "-rw-r--r-- 1 ivan sudo 21880483 Apr 21 09:12 'epoch=17-step=1999.ckpt'\r\n",
      "-rw-r--r-- 1 ivan sudo 21880483 Apr 21 09:15 'epoch=19-step=2199.ckpt'\r\n",
      "-rw-r--r-- 1 ivan sudo 21880483 Apr 21 09:17 'epoch=21-step=2399.ckpt'\r\n",
      "-rw-r--r-- 1 ivan sudo 21880483 Apr 21 09:19 'epoch=22-step=2599.ckpt'\r\n",
      "-rw-r--r-- 1 ivan sudo 21880483 Apr 21 09:22 'epoch=24-step=2799.ckpt'\r\n",
      "-rw-r--r-- 1 ivan sudo 21880483 Apr 21 09:24 'epoch=26-step=2999.ckpt'\r\n",
      "-rw-r--r-- 1 ivan sudo 21880483 Apr 21 08:51 'epoch=3-step=399.ckpt'\r\n",
      "-rw-r--r-- 1 ivan sudo 21880483 Apr 21 08:55 'epoch=5-step=599.ckpt'\r\n",
      "-rw-r--r-- 1 ivan sudo 21880483 Apr 21 08:58 'epoch=7-step=799.ckpt'\r\n",
      "-rw-r--r-- 1 ivan sudo 21880483 Apr 21 09:00 'epoch=8-step=999.ckpt'\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l lightning_logs/version_41/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "baking-pantyhose",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD_ID=0, fold_id_valid=1, model_version=42[epoch=19-step=2199.ckpt]: precision = 0.515, mrr = 0.193, r1 = 0.281\n",
      "FOLD_ID=0, fold_id_valid=1, model_version=42[epoch=21-step=2399.ckpt]: precision = 0.514, mrr = 0.194, r1 = 0.281\n",
      "FOLD_ID=0, fold_id_valid=1, model_version=42[epoch=22-step=2599.ckpt]: precision = 0.513, mrr = 0.193, r1 = 0.281\n",
      "FOLD_ID=0, fold_id_valid=1, model_version=42[epoch=24-step=2799.ckpt]: precision = 0.513, mrr = 0.194, r1 = 0.282\n",
      "FOLD_ID=0, fold_id_valid=1, model_version=42[epoch=26-step=2999.ckpt]: precision = 0.513, mrr = 0.194, r1 = 0.281\n",
      "FOLD_ID=0, fold_id_valid=1, ensemble distance sum: precision = 0.515, mrr = 0.194, r1 = 0.281\n",
      "FOLD_ID=0, fold_id_valid=1, ensemble distance min: precision = 0.514, mrr = 0.194, r1 = 0.281\n",
      "FOLD_ID=0, fold_id_valid=1, ensemble     rank sum: precision = 0.514, mrr = 0.194, r1 = 0.281\n",
      "FOLD_ID=1, fold_id_valid=2, model_version=41[epoch=19-step=2199.ckpt]: precision = 0.495, mrr = 0.195, r1 = 0.280\n",
      "FOLD_ID=1, fold_id_valid=2, model_version=41[epoch=21-step=2399.ckpt]: precision = 0.494, mrr = 0.195, r1 = 0.280\n",
      "FOLD_ID=1, fold_id_valid=2, model_version=41[epoch=22-step=2599.ckpt]: precision = 0.491, mrr = 0.195, r1 = 0.279\n",
      "FOLD_ID=1, fold_id_valid=2, model_version=41[epoch=24-step=2799.ckpt]: precision = 0.501, mrr = 0.195, r1 = 0.281\n",
      "FOLD_ID=1, fold_id_valid=2, model_version=41[epoch=26-step=2999.ckpt]: precision = 0.491, mrr = 0.194, r1 = 0.278\n",
      "FOLD_ID=1, fold_id_valid=2, ensemble distance sum: precision = 0.498, mrr = 0.195, r1 = 0.280\n",
      "FOLD_ID=1, fold_id_valid=2, ensemble distance min: precision = 0.494, mrr = 0.194, r1 = 0.279\n",
      "FOLD_ID=1, fold_id_valid=2, ensemble     rank sum: precision = 0.496, mrr = 0.195, r1 = 0.280\n",
      "FOLD_ID=2, fold_id_valid=3, model_version=43[epoch=19-step=2199.ckpt]: precision = 0.500, mrr = 0.193, r1 = 0.279\n",
      "FOLD_ID=2, fold_id_valid=3, model_version=43[epoch=21-step=2399.ckpt]: precision = 0.500, mrr = 0.193, r1 = 0.279\n",
      "FOLD_ID=2, fold_id_valid=3, model_version=43[epoch=22-step=2599.ckpt]: precision = 0.503, mrr = 0.194, r1 = 0.280\n",
      "FOLD_ID=2, fold_id_valid=3, model_version=43[epoch=24-step=2799.ckpt]: precision = 0.501, mrr = 0.194, r1 = 0.280\n",
      "FOLD_ID=2, fold_id_valid=3, model_version=43[epoch=26-step=2999.ckpt]: precision = 0.502, mrr = 0.194, r1 = 0.280\n",
      "FOLD_ID=2, fold_id_valid=3, ensemble distance sum: precision = 0.503, mrr = 0.194, r1 = 0.280\n",
      "FOLD_ID=2, fold_id_valid=3, ensemble distance min: precision = 0.502, mrr = 0.194, r1 = 0.280\n",
      "FOLD_ID=2, fold_id_valid=3, ensemble     rank sum: precision = 0.503, mrr = 0.194, r1 = 0.280\n"
     ]
    }
   ],
   "source": [
    "for mmap in model_map:\n",
    "    FOLD_ID = mmap['FOLD_ID']\n",
    "    fold_id_test = FOLD_ID\n",
    "    folds_count = len(glob('data/train_matching_*.csv'))\n",
    "    fold_id_valid = (fold_id_test + 1) % folds_count\n",
    "    del fold_id_test\n",
    "    df_matching_valid = pd.read_csv(f'data/train_matching_{fold_id_valid}.csv')\n",
    "    with open(f'data/features_f{FOLD_ID}.pickle', 'rb') as f:\n",
    "        (\n",
    "            _,\n",
    "            features_trx_valid,\n",
    "            _,\n",
    "            _,\n",
    "            _,\n",
    "            features_click_valid,\n",
    "            _,\n",
    "            _,\n",
    "        ) = pickle.load(f)\n",
    "    \n",
    "    valid_dl_trx = torch.utils.data.DataLoader(\n",
    "        PairedDataset(\n",
    "            np.sort(df_matching_valid['bank'].unique()).reshape(-1, 1), \n",
    "            data=[\n",
    "                features_trx_valid,\n",
    "            ],\n",
    "            augmentations=[\n",
    "                augmentation_chain(DropDuplicate('mcc_code', col_new_cnt='c_cnt'), SeqLenLimit(2000)),  # 2000\n",
    "            ],\n",
    "            n_sample=1,\n",
    "        ),\n",
    "        collate_fn=paired_collate_fn,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        batch_size=128,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    valid_dl_click = torch.utils.data.DataLoader(\n",
    "        PairedDataset(\n",
    "            np.sort(df_matching_valid[lambda x: x['rtk'].ne('0')]['rtk'].unique()).reshape(-1, 1),\n",
    "            data=[\n",
    "                features_click_valid,\n",
    "            ],\n",
    "            augmentations=[\n",
    "                augmentation_chain(DropDuplicate('cat_id', col_new_cnt='c_cnt'), SeqLenLimit(5000)),  # 5000\n",
    "            ],\n",
    "            n_sample=1,\n",
    "        ),\n",
    "        collate_fn=paired_collate_fn,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        batch_size=128,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    \n",
    "    vc = ValidationCallback(valid_dl_trx, valid_dl_click, df_matching_valid, \n",
    "                            torch.device('cuda:1'))\n",
    "    \n",
    "    model_version = mmap['mv'][0]\n",
    "    tails = [\n",
    "#         'epoch=1-step=199.ckpt',\n",
    "#         'epoch=3-step=399.ckpt',\n",
    "#         'epoch=5-step=599.ckpt',\n",
    "#         'epoch=7-step=799.ckpt',\n",
    "#         'epoch=8-step=999.ckpt',\n",
    "#         'epoch=10-step=1199.ckpt',\n",
    "#         'epoch=12-step=1399.ckpt',\n",
    "#         'epoch=14-step=1599.ckpt',\n",
    "#         'epoch=15-step=1799.ckpt',\n",
    "#         'epoch=17-step=1999.ckpt',\n",
    "        'epoch=19-step=2199.ckpt',\n",
    "        'epoch=21-step=2399.ckpt',\n",
    "        'epoch=22-step=2599.ckpt',\n",
    "        'epoch=24-step=2799.ckpt',\n",
    "        'epoch=26-step=2999.ckpt',\n",
    "    ]\n",
    "    res = []\n",
    "    for step_tail in tails:\n",
    "        sup_model = PairedModule.load_from_checkpoint(\n",
    "            f'lightning_logs/version_{model_version}/checkpoints/{step_tail}')\n",
    "        \n",
    "        zo = vc.run(sup_model)\n",
    "        precision, mrr, r1 = vc.logits_to_metrics(zo)\n",
    "        res.append(zo)\n",
    "        print(f'FOLD_ID={FOLD_ID}, fold_id_valid={mmap[\"fold_id_valid\"]}, '\n",
    "              f'model_version={model_version}[{step_tail}]: '\n",
    "              f'precision = {precision:.3f}, mrr = {mrr:.3f}, r1 = {r1:.3f}')\n",
    "    \n",
    "    precision, mrr, r1 = vc.logits_to_metrics(torch.stack(res, dim=0).sum(dim=0))\n",
    "    print(f'FOLD_ID={FOLD_ID}, fold_id_valid={mmap[\"fold_id_valid\"]}, ensemble distance sum: '\n",
    "          f'precision = {precision:.3f}, mrr = {mrr:.3f}, r1 = {r1:.3f}')\n",
    "    precision, mrr, r1 = vc.logits_to_metrics(torch.stack(res, dim=0).min(dim=0).values)\n",
    "    print(f'FOLD_ID={FOLD_ID}, fold_id_valid={mmap[\"fold_id_valid\"]}, ensemble distance min: '\n",
    "          f'precision = {precision:.3f}, mrr = {mrr:.3f}, r1 = {r1:.3f}')\n",
    "    \n",
    "    precision, mrr, r1 = vc.logits_to_metrics(torch.stack([-vc.ranks(o) for o in res], dim=0).sum(dim=0))\n",
    "    print(f'FOLD_ID={FOLD_ID}, fold_id_valid={mmap[\"fold_id_valid\"]}, ensemble     rank sum: '\n",
    "          f'precision = {precision:.3f}, mrr = {mrr:.3f}, r1 = {r1:.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-blake",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "minute-chorus",
   "metadata": {},
   "source": [
    "```\n",
    "| Models in ensemble          | FOLD_ID | model count | mean by folds | ensemble | boost  |\n",
    "| --------------------------- | ------- | ----------- | ------------- | -------- | ------ |\n",
    "| sample 85% of train dataset | 0       |        5    | 0.2732        | 0.287    | 1.0505 |\n",
    "| sample 85% of train dataset | 1       |        5    | 0.2748        | 0.288    | 1.0480 |\n",
    "| sample 85% of train dataset | 2       |        5    | 0.2746        | 0.287    | 1.0452 |\n",
    "| --------------------------- | ------- | ----------- | ------------- | -------- | ------ |\n",
    "| full dataset                | 0       |        5    | 0.2798        | 0.294    | 1.0508 |\n",
    "| full dataset                | 1       |        5    | 0.2820        | 0.295    | 1.0461 |\n",
    "| full dataset                | 2       |        5    | 0.2802        | 0.293    | 1.0457 |\n",
    "| --------------------------- | ------- | ----------- | ------------- | -------- | ------ |\n",
    "| model checkpoints           | 0       |        5    | 0.2812        | 0.281    | 0.9993 |\n",
    "| model checkpoints           | 1       |        5    | 0.2796        | 0.280    | 1.0014 |\n",
    "| model checkpoints           | 2       |        5    | 0.2796        | 0.280    | 1.0014 |\n",
    "| --------------------------- | ------- | ----------- | ------------- | -------- | ------ |\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-workstation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vtb",
   "language": "python",
   "name": "vtb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
