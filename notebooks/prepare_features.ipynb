{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "simplified-country",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/kireev/pycharm-deploy/vtb\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-kansas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "joined-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "restricted-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "subtle-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "oriented-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "labeled-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhocon import ConfigFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "outer-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "monthly-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.data_load.iterable_processing.category_size_clip import CategorySizeClip\n",
    "from dltranz.data_load import augmentation_chain\n",
    "from dltranz.data_load.augmentations.seq_len_limit import SeqLenLimit\n",
    "from dltranz.data_load.augmentations.random_slice import RandomSlice\n",
    "\n",
    "from dltranz.seq_encoder import create_encoder\n",
    "\n",
    "from dltranz.metric_learn.sampling_strategies import get_sampling_strategy\n",
    "from dltranz.metric_learn.losses import get_loss\n",
    "\n",
    "from dltranz.tb_interface import get_scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "latter-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vtb_code.data import PairedDataset, paired_collate_fn, DropDuplicate\n",
    "from vtb_code.metrics import PrecisionK, MeanReciprocalRankK, ValidationCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-canadian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accepted-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD_ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "under-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_id_test = FOLD_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fifty-waste",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds_count = len(glob('data/train_matching_*.csv'))\n",
    "folds_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "suburban-detector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fold_id_valid = np.random.choice([i for i in range(folds_count) if i != fold_id_test], size=1)[0]\n",
    "fold_id_valid = (fold_id_test + 1) % folds_count\n",
    "fold_id_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "confidential-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matching_train = pd.concat([pd.read_csv(f'data/train_matching_{i}.csv')\n",
    "                              for i in range(folds_count) \n",
    "                              if i not in (fold_id_test, fold_id_valid)])\n",
    "df_matching_valid = pd.read_csv(f'data/train_matching_{fold_id_valid}.csv')\n",
    "df_matching_test = pd.read_csv(f'data/train_matching_{fold_id_test}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "smooth-experiment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11720, 2930, 2931]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(df) for df in [df_matching_train, df_matching_valid, df_matching_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "related-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trx_types(df):\n",
    "    df['mcc_code'] = df['mcc_code'].astype(str)\n",
    "    df['currency_rk'] = df['currency_rk'].astype(str)\n",
    "    df['event_time'] = pd.to_datetime(df['transaction_dttm']).astype(int) / 1e9\n",
    "    return df[['user_id', 'event_time', 'mcc_code', 'currency_rk', 'transaction_amt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "helpful-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trx_train = pd.concat([trx_types(pd.read_csv(f'data/transactions_{i}.csv'))\n",
    "                              for i in range(folds_count) \n",
    "                              if i not in (fold_id_test, fold_id_valid)])\n",
    "df_trx_valid = trx_types(pd.read_csv(f'data/transactions_{fold_id_valid}.csv'))\n",
    "df_trx_test = trx_types(pd.read_csv(f'data/transactions_{fold_id_test}.csv'))\n",
    "df_trx_puzzle = trx_types(pd.read_csv(f'data/transactions_unmatched.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "raised-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_types(df):\n",
    "    df['event_time'] = pd.to_datetime(df['timestamp']).astype(int) / 1e9\n",
    "    df = pd.merge(df, pd.read_csv('data/click_categories.csv'), on='cat_id')\n",
    "    df['cat_id'] = df['cat_id'].astype(str)\n",
    "    return df[['user_id', 'event_time', 'cat_id', 'level_0', 'level_1', 'level_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "worse-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_click_train = pd.concat([click_types(pd.read_csv(f'data/clickstream_{i}.csv'))\n",
    "                              for i in range(folds_count) \n",
    "                              if i not in (fold_id_test, fold_id_valid)])\n",
    "df_click_valid = click_types(pd.read_csv(f'data/clickstream_{fold_id_valid}.csv'))\n",
    "df_click_test = click_types(pd.read_csv(f'data/clickstream_{fold_id_test}.csv'))\n",
    "df_click_puzzle = click_types(pd.read_csv(f'data/clickstream_unmatched.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-penalty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pursuant-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from dltranz.data_preprocessing.base import DataPreprocessor\n",
    "from dltranz.data_preprocessing.util import pd_hist\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class PandasDataPreprocessor(DataPreprocessor):\n",
    "    \"\"\"Data preprocessor based on pandas.DataFrame\n",
    "\n",
    "    During preprocessing it\n",
    "        * transform `cols_event_time` column with date and time\n",
    "        * encodes category columns `cols_category` into ints;\n",
    "        * apply logarithm transformation to `cols_log_norm' columns;\n",
    "        * groups flat data by `col_id`;\n",
    "        * arranges data into list of dicts with features\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col_id : str\n",
    "        name of column with ids\n",
    "    cols_event_time : str,\n",
    "        name of column with time and date\n",
    "    cols_category : list[str],\n",
    "        list of category columns\n",
    "    cols_log_norm : list[str],\n",
    "        list of columns to be logarithmed\n",
    "    time_transformation: str. Default: 'default'.\n",
    "        type of transformation to be applied to time column\n",
    "    print_dataset_info : bool. Default: False.\n",
    "        If True, print dataset stats during preprocessor fitting and data transformation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 col_id: str,\n",
    "                 cols_event_time: str,\n",
    "                 cols_category: List[str],\n",
    "                 cols_log_norm: List[str],\n",
    "                 time_transformation: str = 'default',\n",
    "                 print_dataset_info: bool = False):\n",
    "\n",
    "        super().__init__(col_id, cols_event_time, cols_category, cols_log_norm)\n",
    "        self.print_dataset_info = print_dataset_info\n",
    "        self.time_transformation = time_transformation\n",
    "\n",
    "    def fit(self, dt, **params):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dt : pandas.DataFrame with flat data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted preprocessor.\n",
    "        \"\"\"\n",
    "        # Reset internal state before fitting\n",
    "        self._reset()\n",
    "\n",
    "        for col in self.cols_category:\n",
    "            pd_col = dt[col].astype(str)\n",
    "            mapping = {k: i + 1 for i, k in enumerate(pd_col.value_counts().index)}\n",
    "            self.cols_category_mapping[col] = mapping\n",
    "\n",
    "            if self.print_dataset_info:\n",
    "                logger.info(f'Encoder stat for \"{col}\":\\ncodes | trx_count\\n{pd_hist(dt[col], col)}')\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, copy=True):\n",
    "        \"\"\"Perform preprocessing.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame with flat data\n",
    "        copy : bool, default=None\n",
    "            Copy the input X or not.\n",
    "        Returns\n",
    "        -------\n",
    "        features : List of dicts grouped by col_id.\n",
    "        \"\"\"\n",
    "        self.check_is_fitted()\n",
    "        df_data = df.copy() if copy else df\n",
    "\n",
    "        if self.print_dataset_info:\n",
    "            logger.info(f'Found {df_data[self.col_id].nunique()} unique ids')\n",
    "\n",
    "        # event_time mapping\n",
    "        if self.time_transformation == 'none':\n",
    "            pass\n",
    "        elif self.time_transformation == 'default':\n",
    "            df_data = self._td_default(df_data, self.cols_event_time)\n",
    "        elif self.time_transformation == 'float':\n",
    "            df_data = self._td_float(df_data, self.cols_event_time)\n",
    "        elif self.time_transformation == 'gender':\n",
    "            df_data = self._td_gender(df_data, self.cols_event_time)\n",
    "        else:\n",
    "            raise NotImplementedError(f'Unknown type of data transformation: \"{self.time_transformation}\"')\n",
    "\n",
    "        for col in self.cols_category:\n",
    "            if col not in self.cols_category_mapping:\n",
    "                raise KeyError(f\"column {col} isn't in fitted category columns\")\n",
    "            pd_col = df_data[col].astype(str)\n",
    "            df_data[col] = pd_col.map(self.cols_category_mapping[col]) \\\n",
    "                .fillna(max(self.cols_category_mapping[col].values()))\n",
    "            if self.print_dataset_info:\n",
    "                logger.info(f'Encoder stat for \"{col}\":\\ncodes | trx_count\\n{pd_hist(df_data[col], col)}')\n",
    "\n",
    "        for col in self.cols_log_norm:\n",
    "            df_data[col] = np.log1p(abs(df_data[col])) * np.sign(df_data[col])\n",
    "            df_data[col] /= abs(df_data[col]).max()\n",
    "            if self.print_dataset_info:\n",
    "                logger.info(f'Encoder stat for \"{col}\":\\ncodes | trx_count\\n{pd_hist(df_data[col], col)}')\n",
    "\n",
    "        if self.print_dataset_info:\n",
    "            df = df_data.groupby(self.col_id)['event_time'].count()\n",
    "            logger.info(f'Trx count per clients:\\nlen(trx_list) | client_count\\n{pd_hist(df, \"trx_count\")}')\n",
    "\n",
    "        # column filter\n",
    "        used_columns = [col for col in df_data.columns\n",
    "                        if col in self.cols_category + self.cols_log_norm + ['event_time', self.col_id]]\n",
    "\n",
    "        logger.info('Feature collection in progress ...')\n",
    "        features = df_data[used_columns] \\\n",
    "            .assign(et_index=lambda x: x['event_time']) \\\n",
    "            .set_index([self.col_id, 'et_index']).sort_index() \\\n",
    "            .groupby(self.col_id).apply(lambda x: {k: np.array(v) for k, v in x.to_dict(orient='list').items()}) \\\n",
    "            .rename('feature_arrays').reset_index().to_dict(orient='records')\n",
    "\n",
    "        def squeeze(rec):\n",
    "            return {self.col_id: rec[self.col_id], **rec['feature_arrays']}\n",
    "        features = [squeeze(r) for r in features]\n",
    "\n",
    "        if self.print_dataset_info:\n",
    "            feature_names = list(features[0].keys())\n",
    "            logger.info(f'Feature names: {feature_names}')\n",
    "\n",
    "        logger.info(f'Prepared features for {len(features)} clients')\n",
    "        return features\n",
    "\n",
    "    @staticmethod\n",
    "    def _td_default(df, cols_event_time):\n",
    "        df_event_time = df[cols_event_time].drop_duplicates()\n",
    "        df_event_time = df_event_time.sort_values(cols_event_time)\n",
    "        df_event_time['event_time'] = np.arange(len(df_event_time))\n",
    "        df = pd.merge(df, df_event_time, on=cols_event_time)\n",
    "        logger.info('Default time transformation')\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _td_float(df, col_event_time):\n",
    "        df['event_time'] = df[col_event_time].astype(float)\n",
    "        logger.info('To-float time transformation')\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _td_gender(df, col_event_time):\n",
    "        \"\"\"Gender-dataset-like transformation\n",
    "\n",
    "        'd hh:mm:ss' -> float where integer part is day number and fractional part is seconds from day begin\n",
    "        '1 00:00:00' -> 1.0\n",
    "        '1 12:00:00' -> 1.5\n",
    "        '1 01:00:00' -> 1 + 1 / 24\n",
    "        '2 23:59:59' -> 1.99\n",
    "        '432 12:00:00' -> 432.5\n",
    "\n",
    "        :param df:\n",
    "        :param col_event_time:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        padded_time = df[col_event_time].str.pad(15, 'left', '0')\n",
    "        day_part = padded_time.str[:6].astype(float)\n",
    "        time_part = pd.to_datetime(padded_time.str[7:], format='%H:%M:%S').values.astype(int) // 1e9\n",
    "        time_part = time_part % (24 * 60 * 60) / (24 * 60 * 60)\n",
    "        df['event_time'] = day_part + time_part\n",
    "        logger.info('Gender-dataset-like time transformation')\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "together-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_trx = PandasDataPreprocessor(\n",
    "    col_id='user_id',\n",
    "    cols_event_time='event_time',\n",
    "    time_transformation='none',\n",
    "    cols_category=[\"mcc_code\", \"currency_rk\"],\n",
    "    cols_log_norm=[\"transaction_amt\"],\n",
    "    print_dataset_info=False,\n",
    ")\n",
    "\n",
    "preprocessor_click = PandasDataPreprocessor(\n",
    "    col_id='user_id',\n",
    "    cols_event_time='event_time',\n",
    "    time_transformation='none',\n",
    "    cols_category=['cat_id', 'level_0', 'level_1', 'level_2'],\n",
    "    cols_log_norm=[],\n",
    "    print_dataset_info=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-campaign",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "oriented-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_max_size_trx = {\n",
    "    'mcc_code': 350,\n",
    "    'currency_rk': 5,\n",
    "}\n",
    "category_max_size_click = {\n",
    "    'cat_id': 400,\n",
    "    'level_0': 400,\n",
    "    'level_1': 400,\n",
    "    'level_2': 400,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "affiliated-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trx_to_torch(seq):\n",
    "    seq = CategorySizeClip(category_max_size_trx)(seq)\n",
    "    for x in seq:\n",
    "        yield x['user_id'], {\n",
    "            'event_time': torch.from_numpy(x['event_time']).float(),\n",
    "            'mcc_code': torch.from_numpy(x['mcc_code']).int(),\n",
    "            'currency_rk': torch.from_numpy(x['currency_rk']).int(),\n",
    "            'transaction_amt': torch.from_numpy(x['transaction_amt']).float(),\n",
    "        }\n",
    "\n",
    "def click_to_torch(seq):\n",
    "    seq = CategorySizeClip(category_max_size_click)(seq)\n",
    "    for x in seq:\n",
    "        yield x['user_id'], {\n",
    "            'event_time': torch.from_numpy(x['event_time']).float(),\n",
    "            'cat_id': torch.from_numpy(x['cat_id']).int(),\n",
    "            'level_0': torch.from_numpy(x['level_0']).int(),\n",
    "            'level_1': torch.from_numpy(x['level_1']).int(),\n",
    "            'level_2': torch.from_numpy(x['level_2']).int(),\n",
    "\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "upset-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_trx_train = dict(trx_to_torch(preprocessor_trx.fit_transform(df_trx_train)))\n",
    "features_trx_valid = dict(trx_to_torch(preprocessor_trx.transform(df_trx_valid)))\n",
    "features_trx_test = dict(trx_to_torch(preprocessor_trx.transform(df_trx_test)))\n",
    "features_trx_puzzle = dict(trx_to_torch(preprocessor_trx.transform(df_trx_puzzle)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "horizontal-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_click_train = dict(click_to_torch(preprocessor_click.fit_transform(df_click_train)))\n",
    "features_click_valid = dict(click_to_torch(preprocessor_click.transform(df_click_valid)))\n",
    "features_click_test = dict(click_to_torch(preprocessor_click.transform(df_click_test)))\n",
    "features_click_puzzle = dict(click_to_torch(preprocessor_click.transform(df_click_puzzle)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-defeat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "parental-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/features_f{FOLD_ID}.pickle', 'wb') as f:\n",
    "    pickle.dump((\n",
    "        features_trx_train,\n",
    "        features_trx_valid,\n",
    "        features_trx_test,\n",
    "        features_trx_puzzle,\n",
    "        features_click_train,\n",
    "        features_click_valid,\n",
    "        features_click_test,\n",
    "        features_click_puzzle,\n",
    "    ), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "progressive-continuity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.6 s, sys: 4.18 s, total: 17.8 s\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(f'data/features_f{FOLD_ID}.pickle', 'rb') as f:\n",
    "    (\n",
    "        features_trx_train,\n",
    "        features_trx_valid,\n",
    "        features_trx_test,\n",
    "        features_trx_puzzle,\n",
    "        features_click_train,\n",
    "        features_click_valid,\n",
    "        features_click_test,\n",
    "        features_click_puzzle,\n",
    "    ) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-airplane",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vtb",
   "language": "python",
   "name": "vtb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
