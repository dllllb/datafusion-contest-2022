{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "836d1120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69307/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "simplified-country",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nikita/ML/work_repo/vtb_competition/vtb_data_fusion_contest\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "restricted-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pyhocon import ConfigFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accepted-attendance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FOLD_ID = 1\n",
    "\n",
    "fold_id_test = FOLD_ID\n",
    "\n",
    "folds_count = len(glob('data/train_matching_*.csv'))\n",
    "folds_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "under-straight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fold_id_valid = np.random.choice([i for i in range(folds_count) if i != fold_id_test], size=1)[0]\n",
    "fold_id_valid = (fold_id_test + 1) % folds_count\n",
    "fold_id_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "confidential-proof",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11721, 2930, 2930]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_matching_train = pd.concat([pd.read_csv(f'data/train_matching_{i}.csv')\n",
    "                              for i in range(folds_count) \n",
    "                              if i not in (fold_id_test, fold_id_valid)])\n",
    "df_matching_valid = pd.read_csv(f'data/train_matching_{fold_id_valid}.csv')\n",
    "df_matching_test = pd.read_csv(f'data/train_matching_{fold_id_test}.csv')\n",
    "\n",
    "[len(df) for df in [df_matching_train, df_matching_valid, df_matching_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-experiment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5175ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_types(df):\n",
    "    df['cat_id'] = df['cat_id'].astype(str)\n",
    "    return df[['user_id', 'timestamp', 'cat_id']]\n",
    "\n",
    "def click_pivot(df):\n",
    "    clickstream_embed = df.pivot_table(index = 'user_id', \n",
    "                            values=['timestamp'],\n",
    "                            columns=['cat_id'],\n",
    "                            aggfunc=['count']).fillna(0)\n",
    "    clickstream_embed.columns = [f'{str(i[0])}-{str(i[2])}' for i in clickstream_embed.columns]\n",
    "    clickstream_embed.loc['0'] = np.empty(len(clickstream_embed.columns))\n",
    "\n",
    "    dtype_clickstream = list()\n",
    "    for x in clickstream_embed.dtypes.tolist():\n",
    "        if x=='int64':\n",
    "            dtype_clickstream.append('int16')\n",
    "        elif(x=='float64'):\n",
    "            dtype_clickstream.append('float32')\n",
    "        else:\n",
    "            dtype_clickstream.append('object')\n",
    "\n",
    "    dtype_clickstream = dict(zip(clickstream_embed.columns.tolist(), dtype_clickstream))\n",
    "    clickstream_embed = clickstream_embed.astype(dtype_clickstream)\n",
    "    clickstream_embed.reset_index(drop=False, inplace=True)\n",
    "    return clickstream_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3dc0d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 39s, sys: 18.2 s, total: 1min 57s\n",
      "Wall time: 2min 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69307/1841381636.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  clickstream_embed.reset_index(drop=False, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_click = pd.read_csv(f'data/clickstream.csv')\n",
    "df_click = click_types(df_click)\n",
    "df_click = click_pivot(df_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8adb4a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9782, 403) (2446, 403) (2446, 403)\n"
     ]
    }
   ],
   "source": [
    "df_click_train = df_click[lambda x: x['user_id'].isin(df_matching_train['rtk'].values)]\n",
    "df_click_valid = df_click[lambda x: x['user_id'].isin(df_matching_valid['rtk'].values)]\n",
    "df_click_test = df_click[lambda x: x['user_id'].isin(df_matching_test['rtk'].values)]\n",
    "\n",
    "print(df_click_train.shape, df_click_valid.shape, df_click_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2815fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac890524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20bc6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trx_types(df):\n",
    "    df['mcc_code'] = df['mcc_code'].astype(str)\n",
    "    df['currency_rk'] = df['currency_rk'].astype(str)\n",
    "    df['event_time'] = pd.to_datetime(df['transaction_dttm']).astype(int) / 1e9\n",
    "    return df[['user_id', 'event_time', 'mcc_code', 'currency_rk', 'transaction_amt']]\n",
    "\n",
    "def trx_pivot(df):\n",
    "    bankclient_embed = df.pivot_table(index = 'user_id', \n",
    "                        values=['transaction_amt'],\n",
    "                        columns=['mcc_code'],\n",
    "                        aggfunc=['sum','mean', 'count']).fillna(0)\n",
    "    bankclient_embed.columns = [f'{str(i[0])}-{str(i[2])}' for i in bankclient_embed.columns]\n",
    "    \n",
    "    dtype_bankclient = list()\n",
    "    for x in bankclient_embed.dtypes.tolist():\n",
    "        if x=='int64':\n",
    "            dtype_bankclient.append('int16')\n",
    "        elif(x=='float64'):\n",
    "            dtype_bankclient.append('float32')\n",
    "        else:\n",
    "            dtype_bankclient.append('object')\n",
    "    \n",
    "    dtype_bankclient = dict(zip(bankclient_embed.columns.tolist(), dtype_bankclient))\n",
    "    bankclient_embed = bankclient_embed.astype(dtype_bankclient)\n",
    "    bankclient_embed.reset_index(drop=False, inplace=True)\n",
    "    return bankclient_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ef414e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69307/3274736351.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bankclient_embed.reset_index(drop=False, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11721, 1159) (2930, 1159) (2930, 1159)\n",
      "CPU times: user 27.8 s, sys: 3.85 s, total: 31.6 s\n",
      "Wall time: 33.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_trx = pd.read_csv(f'data/transactions.csv')\n",
    "df_trx = trx_types(df_trx)\n",
    "df_trx = trx_pivot(df_trx)\n",
    "\n",
    "df_trx_train = df_trx[lambda x: x['user_id'].isin(df_matching_train['bank'].values)]\n",
    "df_trx_valid = df_trx[lambda x: x['user_id'].isin(df_matching_valid['bank'].values)]\n",
    "df_trx_test = df_trx[lambda x: x['user_id'].isin(df_matching_test['bank'].values)]\n",
    "\n",
    "print(df_trx_train.shape, df_trx_valid.shape, df_trx_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb238769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((234420, 1563), (58600, 1563))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_dataset(df_matching, df_trx, df_click, neg_count=19):\n",
    "    positive = pd.merge(df_matching, df_trx, left_on='bank', right_on='user_id').drop(columns='user_id')\n",
    "    positive = pd.merge(positive, df_click, left_on='rtk', right_on='user_id').drop(columns='user_id')\n",
    "    positive['target'] = 1\n",
    "\n",
    "    rtks = df_click['user_id'].unique()\n",
    "\n",
    "    negative = pd.DataFrame(data=df_trx['user_id'].values, columns=['bank'])\n",
    "    negative['rtk'] = negative['bank'].apply(lambda x: np.random.choice(rtks, size=neg_count))\n",
    "    negative = negative.explode('rtk')\n",
    "\n",
    "    negative = pd.merge(negative, df_trx, left_on='bank', right_on='user_id').drop(columns='user_id')\n",
    "    negative = pd.merge(negative, df_click, left_on='rtk', right_on='user_id').drop(columns='user_id')\n",
    "    negative['target'] = 0\n",
    "\n",
    "    dataset = pd.concat([positive, negative]).sample(frac=1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train = prepare_dataset(df_matching_train, df_trx_train, df_click_train)\n",
    "valid = prepare_dataset(df_matching_valid, df_trx_valid, df_click_valid)\n",
    "\n",
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8086f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1aec26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ac2a8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikita/.local/share/virtualenvs/pytorch-lifestream-VR0LSEAW/lib/python3.8/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/nikita/.local/share/virtualenvs/pytorch-lifestream-VR0LSEAW/lib/python3.8/site-packages/lightgbm/basic.py:1702: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttraining's auc: 0.817466\tvalid_1's auc: 0.689166\n",
      "[100]\ttraining's auc: 0.844353\tvalid_1's auc: 0.699085\n",
      "[150]\ttraining's auc: 0.866751\tvalid_1's auc: 0.709099\n",
      "[200]\ttraining's auc: 0.884123\tvalid_1's auc: 0.718938\n",
      "[250]\ttraining's auc: 0.896529\tvalid_1's auc: 0.723936\n",
      "[300]\ttraining's auc: 0.907376\tvalid_1's auc: 0.728698\n",
      "[350]\ttraining's auc: 0.918215\tvalid_1's auc: 0.734661\n",
      "[400]\ttraining's auc: 0.928936\tvalid_1's auc: 0.739422\n",
      "[450]\ttraining's auc: 0.937237\tvalid_1's auc: 0.742933\n",
      "[500]\ttraining's auc: 0.94502\tvalid_1's auc: 0.745882\n",
      "[550]\ttraining's auc: 0.951659\tvalid_1's auc: 0.749352\n",
      "[600]\ttraining's auc: 0.957092\tvalid_1's auc: 0.752488\n",
      "[650]\ttraining's auc: 0.962384\tvalid_1's auc: 0.754494\n",
      "[700]\ttraining's auc: 0.967116\tvalid_1's auc: 0.75771\n",
      "[750]\ttraining's auc: 0.970583\tvalid_1's auc: 0.760454\n",
      "[800]\ttraining's auc: 0.973719\tvalid_1's auc: 0.762321\n",
      "[850]\ttraining's auc: 0.976573\tvalid_1's auc: 0.764256\n",
      "[900]\ttraining's auc: 0.97893\tvalid_1's auc: 0.765839\n",
      "[950]\ttraining's auc: 0.980968\tvalid_1's auc: 0.767431\n",
      "[1000]\ttraining's auc: 0.983123\tvalid_1's auc: 0.769238\n",
      "[1050]\ttraining's auc: 0.984589\tvalid_1's auc: 0.770063\n",
      "[1100]\ttraining's auc: 0.985921\tvalid_1's auc: 0.771713\n",
      "[1150]\ttraining's auc: 0.987293\tvalid_1's auc: 0.772835\n",
      "[1200]\ttraining's auc: 0.988424\tvalid_1's auc: 0.773844\n",
      "[1250]\ttraining's auc: 0.989627\tvalid_1's auc: 0.775422\n",
      "[1300]\ttraining's auc: 0.990623\tvalid_1's auc: 0.776406\n",
      "[1350]\ttraining's auc: 0.991414\tvalid_1's auc: 0.777687\n",
      "[1400]\ttraining's auc: 0.992201\tvalid_1's auc: 0.778397\n",
      "[1450]\ttraining's auc: 0.992893\tvalid_1's auc: 0.778794\n",
      "[1500]\ttraining's auc: 0.993518\tvalid_1's auc: 0.779661\n",
      "[1550]\ttraining's auc: 0.994065\tvalid_1's auc: 0.780366\n",
      "[1600]\ttraining's auc: 0.994613\tvalid_1's auc: 0.781594\n",
      "[1650]\ttraining's auc: 0.995022\tvalid_1's auc: 0.782364\n",
      "[1700]\ttraining's auc: 0.995413\tvalid_1's auc: 0.783096\n",
      "[1750]\ttraining's auc: 0.995726\tvalid_1's auc: 0.783729\n",
      "[1800]\ttraining's auc: 0.996065\tvalid_1's auc: 0.784032\n",
      "[1850]\ttraining's auc: 0.996406\tvalid_1's auc: 0.784518\n",
      "[1900]\ttraining's auc: 0.996696\tvalid_1's auc: 0.784934\n",
      "[1950]\ttraining's auc: 0.996925\tvalid_1's auc: 0.7852\n",
      "[2000]\ttraining's auc: 0.997183\tvalid_1's auc: 0.785311\n",
      "[2050]\ttraining's auc: 0.997365\tvalid_1's auc: 0.785501\n",
      "[2100]\ttraining's auc: 0.997548\tvalid_1's auc: 0.785654\n",
      "[2150]\ttraining's auc: 0.997726\tvalid_1's auc: 0.786149\n",
      "[2200]\ttraining's auc: 0.997895\tvalid_1's auc: 0.786578\n",
      "[2250]\ttraining's auc: 0.99805\tvalid_1's auc: 0.786469\n",
      "[2300]\ttraining's auc: 0.998188\tvalid_1's auc: 0.786503\n",
      "[2350]\ttraining's auc: 0.998321\tvalid_1's auc: 0.786673\n",
      "[2400]\ttraining's auc: 0.998429\tvalid_1's auc: 0.787107\n",
      "[2450]\ttraining's auc: 0.998553\tvalid_1's auc: 0.787074\n",
      "[2500]\ttraining's auc: 0.99866\tvalid_1's auc: 0.787363\n",
      "[2550]\ttraining's auc: 0.998749\tvalid_1's auc: 0.787529\n",
      "[2600]\ttraining's auc: 0.998843\tvalid_1's auc: 0.787704\n",
      "[2650]\ttraining's auc: 0.99892\tvalid_1's auc: 0.787798\n",
      "[2700]\ttraining's auc: 0.999004\tvalid_1's auc: 0.787828\n",
      "[2750]\ttraining's auc: 0.999071\tvalid_1's auc: 0.787678\n",
      "[2800]\ttraining's auc: 0.999129\tvalid_1's auc: 0.787947\n",
      "[2850]\ttraining's auc: 0.999186\tvalid_1's auc: 0.788203\n",
      "[2900]\ttraining's auc: 0.999245\tvalid_1's auc: 0.788171\n",
      "[2950]\ttraining's auc: 0.999294\tvalid_1's auc: 0.788463\n",
      "[3000]\ttraining's auc: 0.999344\tvalid_1's auc: 0.788389\n",
      "[3050]\ttraining's auc: 0.999386\tvalid_1's auc: 0.788199\n",
      "[3100]\ttraining's auc: 0.999424\tvalid_1's auc: 0.788374\n",
      "[3150]\ttraining's auc: 0.999469\tvalid_1's auc: 0.788437\n",
      "[3200]\ttraining's auc: 0.999503\tvalid_1's auc: 0.78846\n",
      "[3250]\ttraining's auc: 0.999533\tvalid_1's auc: 0.788557\n",
      "[3300]\ttraining's auc: 0.999561\tvalid_1's auc: 0.788541\n",
      "[3350]\ttraining's auc: 0.99959\tvalid_1's auc: 0.788707\n",
      "[3400]\ttraining's auc: 0.999618\tvalid_1's auc: 0.788695\n",
      "[3450]\ttraining's auc: 0.999641\tvalid_1's auc: 0.788594\n",
      "[3500]\ttraining's auc: 0.99966\tvalid_1's auc: 0.788519\n",
      "[3550]\ttraining's auc: 0.999678\tvalid_1's auc: 0.78848\n",
      "[3600]\ttraining's auc: 0.999697\tvalid_1's auc: 0.788503\n",
      "[3650]\ttraining's auc: 0.999715\tvalid_1's auc: 0.788189\n",
      "[3700]\ttraining's auc: 0.999732\tvalid_1's auc: 0.788276\n",
      "[3750]\ttraining's auc: 0.999745\tvalid_1's auc: 0.788213\n",
      "[3800]\ttraining's auc: 0.999758\tvalid_1's auc: 0.788355\n",
      "[3850]\ttraining's auc: 0.999772\tvalid_1's auc: 0.788465\n",
      "[3900]\ttraining's auc: 0.999783\tvalid_1's auc: 0.788456\n",
      "[3950]\ttraining's auc: 0.999794\tvalid_1's auc: 0.788658\n",
      "[4000]\ttraining's auc: 0.999804\tvalid_1's auc: 0.788772\n",
      "[4050]\ttraining's auc: 0.999814\tvalid_1's auc: 0.788932\n",
      "[4100]\ttraining's auc: 0.999822\tvalid_1's auc: 0.788756\n",
      "[4150]\ttraining's auc: 0.999831\tvalid_1's auc: 0.788651\n",
      "[4200]\ttraining's auc: 0.99984\tvalid_1's auc: 0.788707\n",
      "[4250]\ttraining's auc: 0.999847\tvalid_1's auc: 0.788741\n",
      "[4300]\ttraining's auc: 0.999853\tvalid_1's auc: 0.788791\n",
      "[4350]\ttraining's auc: 0.999859\tvalid_1's auc: 0.788704\n",
      "[4400]\ttraining's auc: 0.999864\tvalid_1's auc: 0.788567\n",
      "[4450]\ttraining's auc: 0.99987\tvalid_1's auc: 0.788574\n",
      "[4500]\ttraining's auc: 0.999875\tvalid_1's auc: 0.788703\n",
      "[4550]\ttraining's auc: 0.99988\tvalid_1's auc: 0.788865\n",
      "[4600]\ttraining's auc: 0.999884\tvalid_1's auc: 0.788843\n",
      "[4650]\ttraining's auc: 0.999888\tvalid_1's auc: 0.788942\n",
      "[4700]\ttraining's auc: 0.999893\tvalid_1's auc: 0.788968\n",
      "[4750]\ttraining's auc: 0.999896\tvalid_1's auc: 0.788859\n",
      "[4800]\ttraining's auc: 0.999901\tvalid_1's auc: 0.788895\n",
      "[4850]\ttraining's auc: 0.999904\tvalid_1's auc: 0.788959\n",
      "[4900]\ttraining's auc: 0.999908\tvalid_1's auc: 0.78902\n",
      "[4950]\ttraining's auc: 0.999911\tvalid_1's auc: 0.788952\n",
      "[5000]\ttraining's auc: 0.999914\tvalid_1's auc: 0.78899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.789"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "drop_columns = ['bank', 'rtk', 'target']\n",
    "TARGET = 'target'\n",
    "CAT_FEATURES = []\n",
    "\n",
    "params = dict(\n",
    "    objective='binary',\n",
    "    metric='auc',\n",
    "    n_estimators=5000,\n",
    "    boosting_type='gbdt',\n",
    "    learning_rate=0.01,\n",
    "    subsample=0.75,\n",
    "    subsample_freq=1,\n",
    "    feature_fraction=0.75,\n",
    "    max_depth=8,\n",
    "    lambda_l1=0.5,\n",
    "    lambda_l2=0.5,\n",
    "    num_leaves=128,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "train_data = lgb.Dataset(train.drop(columns=drop_columns),\n",
    "                                 label=train[TARGET],\n",
    "                                 categorical_feature=CAT_FEATURES)\n",
    "\n",
    "valid_data = lgb.Dataset(valid.drop(columns=drop_columns),\n",
    "                         label=valid[TARGET],\n",
    "                         categorical_feature=CAT_FEATURES)\n",
    "\n",
    "lgb_model = lgb.train(params, train_data, valid_sets=[train_data, valid_data], verbose_eval=50)\n",
    "preds = lgb_model.predict(valid.drop(columns=drop_columns))\n",
    "\n",
    "metric_value = roc_auc_score(valid[TARGET], preds)\n",
    "round(metric_value, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda5256",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trx_valid.set_index('user_id', inplace=True)\n",
    "df_click_valid.set_index('user_id', inplace=True)\n",
    "df_matching_valid.set_index('bank', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "501e0751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(df_trx, df_click, model, model_features, batch_size=200):\n",
    "\n",
    "    list_of_rtk = list(df_click.index.unique())\n",
    "    list_of_bank = list(df_trx.index.unique())\n",
    "\n",
    "    submission = pd.DataFrame(list_of_bank, columns=['bank'])\n",
    "    submission['rtk'] = submission['bank'].apply(lambda x: list_of_rtk)\n",
    "\n",
    "    num_of_batches = int((len(list_of_bank))/batch_size)+1\n",
    "    submission_ready = []\n",
    "\n",
    "    for i in range(num_of_batches):\n",
    "        bank_ids = list_of_bank[(i*batch_size):((i+1)*batch_size)]\n",
    "        if len(bank_ids) != 0:\n",
    "            part_of_submit = submission[submission['bank'].isin(bank_ids)].explode('rtk')\n",
    "            part_of_submit = part_of_submit.merge(df_trx, how='left', left_on='bank', right_index=True\n",
    "                                        ).merge(df_click, how='left', left_on='rtk', right_index=True).fillna(0)\n",
    "\n",
    "            for feature in model_features:\n",
    "                if feature not in part_of_submit.columns:\n",
    "                    part_of_submit[i] = 0\n",
    "\n",
    "            part_of_submit['predicts'] = model.predict(part_of_submit[model_features])\n",
    "            part_of_submit = part_of_submit[['bank', 'rtk', 'predicts']]\n",
    "\n",
    "            part_of_submit = part_of_submit.sort_values(by=['bank', 'predicts'], ascending=False).reset_index(drop=True)\n",
    "            part_of_submit = part_of_submit.pivot_table(index='bank', values='rtk', aggfunc=list)\n",
    "            part_of_submit['rtk'] = part_of_submit['rtk'].apply(lambda x: x[:100])\n",
    "            part_of_submit['bank'] = part_of_submit.index\n",
    "            part_of_submit = part_of_submit[['bank', 'rtk']]\n",
    "            submission_ready.extend(part_of_submit.values)\n",
    "\n",
    "    submission_final = np.array(submission_ready, dtype=object)\n",
    "    return submission_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8f111a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 40min 42s, sys: 43.4 s, total: 1h 41min 26s\n",
      "Wall time: 14min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preds = inference(df_trx_valid, df_click_valid, lgb_model, lgb_model.feature_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a5f48926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre: 0.356, mrr: 0.174, r1: 0.234\n",
      "CPU times: user 107 ms, sys: 0 ns, total: 107 ms\n",
      "Wall time: 106 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def score(preds, matching):\n",
    "    pre = 0.0\n",
    "    mrr = 0.0\n",
    "\n",
    "    for ix_bank, pred in preds:\n",
    "        match = matching.loc[ix_bank]['rtk']\n",
    "        d = match in pred\n",
    "        if d:\n",
    "            pre += 1\n",
    "            mrr += 1 / (1 + pred.index(match))\n",
    "\n",
    "    pre /= len(preds)\n",
    "    mrr /= len(preds)\n",
    "    r1 = 2 * pre * mrr / (pre + mrr)\n",
    "\n",
    "    return pre, mrr, r1\n",
    "\n",
    "pre, mrr, r1 = score(preds, df_matching_valid)\n",
    "\n",
    "print(f'pre: {pre:.3f}, mrr: {mrr:.3f}, r1: {r1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016634c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536adf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
