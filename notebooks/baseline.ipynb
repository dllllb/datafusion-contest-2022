{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "836d1120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8432/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "simplified-country",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nikita/ML/work_repo/vtb_competition/vtb_data_fusion_contest\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "restricted-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pyhocon import ConfigFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accepted-attendance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FOLD_ID = 1\n",
    "\n",
    "fold_id_test = FOLD_ID\n",
    "\n",
    "folds_count = len(glob('data/train_matching_*.csv'))\n",
    "folds_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "under-straight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fold_id_valid = np.random.choice([i for i in range(folds_count) if i != fold_id_test], size=1)[0]\n",
    "fold_id_valid = (fold_id_test + 1) % folds_count\n",
    "fold_id_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "confidential-proof",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11721, 2930, 2930]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_matching_train = pd.concat([pd.read_csv(f'data/train_matching_{i}.csv')\n",
    "                              for i in range(folds_count) \n",
    "                              if i not in (fold_id_test, fold_id_valid)])\n",
    "df_matching_valid = pd.read_csv(f'data/train_matching_{fold_id_valid}.csv')\n",
    "df_matching_test = pd.read_csv(f'data/train_matching_{fold_id_test}.csv')\n",
    "\n",
    "[len(df) for df in [df_matching_train, df_matching_valid, df_matching_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-experiment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5175ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_types(df):\n",
    "    df['cat_id'] = df['cat_id'].astype(str)\n",
    "    return df[['user_id', 'timestamp', 'cat_id']]\n",
    "\n",
    "def click_pivot(df):\n",
    "    clickstream_embed = df.pivot_table(index = 'user_id', \n",
    "                            values=['timestamp'],\n",
    "                            columns=['cat_id'],\n",
    "                            aggfunc=['count']).fillna(0)\n",
    "    clickstream_embed.columns = [f'{str(i[0])}-{str(i[2])}' for i in clickstream_embed.columns]\n",
    "    clickstream_embed.loc['0'] = np.empty(len(clickstream_embed.columns))\n",
    "\n",
    "    dtype_clickstream = list()\n",
    "    for x in clickstream_embed.dtypes.tolist():\n",
    "        if x=='int64':\n",
    "            dtype_clickstream.append('int16')\n",
    "        elif(x=='float64'):\n",
    "            dtype_clickstream.append('float32')\n",
    "        else:\n",
    "            dtype_clickstream.append('object')\n",
    "\n",
    "    dtype_clickstream = dict(zip(clickstream_embed.columns.tolist(), dtype_clickstream))\n",
    "    clickstream_embed = clickstream_embed.astype(dtype_clickstream)\n",
    "    clickstream_embed.reset_index(drop=False, inplace=True)\n",
    "    return clickstream_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3dc0d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 20 s, total: 2min 6s\n",
      "Wall time: 2min 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8432/1841381636.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  clickstream_embed.reset_index(drop=False, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_click = pd.read_csv(f'data/clickstream.csv')\n",
    "df_click = click_types(df_click)\n",
    "df_click = click_pivot(df_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8adb4a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9782, 403) (2446, 403) (2446, 403)\n"
     ]
    }
   ],
   "source": [
    "df_click_train = df_click[lambda x: x['user_id'].isin(df_matching_train['rtk'].values)]\n",
    "df_click_valid = df_click[lambda x: x['user_id'].isin(df_matching_valid['rtk'].values)]\n",
    "df_click_test = df_click[lambda x: x['user_id'].isin(df_matching_test['rtk'].values)]\n",
    "\n",
    "print(df_click_train.shape, df_click_valid.shape, df_click_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2815fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac890524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20bc6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trx_types(df):\n",
    "    df['mcc_code'] = df['mcc_code'].astype(str)\n",
    "    df['currency_rk'] = df['currency_rk'].astype(str)\n",
    "    df['event_time'] = pd.to_datetime(df['transaction_dttm']).astype(int) / 1e9\n",
    "    return df[['user_id', 'event_time', 'mcc_code', 'currency_rk', 'transaction_amt']]\n",
    "\n",
    "def trx_pivot(df):\n",
    "    bankclient_embed = df.pivot_table(index = 'user_id', \n",
    "                        values=['transaction_amt'],\n",
    "                        columns=['mcc_code'],\n",
    "                        aggfunc=['sum','mean', 'count']).fillna(0)\n",
    "    bankclient_embed.columns = [f'{str(i[0])}-{str(i[2])}' for i in bankclient_embed.columns]\n",
    "    \n",
    "    dtype_bankclient = list()\n",
    "    for x in bankclient_embed.dtypes.tolist():\n",
    "        if x=='int64':\n",
    "            dtype_bankclient.append('int16')\n",
    "        elif(x=='float64'):\n",
    "            dtype_bankclient.append('float32')\n",
    "        else:\n",
    "            dtype_bankclient.append('object')\n",
    "    \n",
    "    dtype_bankclient = dict(zip(bankclient_embed.columns.tolist(), dtype_bankclient))\n",
    "    bankclient_embed = bankclient_embed.astype(dtype_bankclient)\n",
    "    bankclient_embed.reset_index(drop=False, inplace=True)\n",
    "    return bankclient_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ef414e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8432/3274736351.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  bankclient_embed.reset_index(drop=False, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11721, 1159) (2930, 1159) (2930, 1159)\n",
      "CPU times: user 31.9 s, sys: 3.82 s, total: 35.7 s\n",
      "Wall time: 36.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_trx = pd.read_csv(f'data/transactions.csv')\n",
    "df_trx = trx_types(df_trx)\n",
    "df_trx = trx_pivot(df_trx)\n",
    "\n",
    "df_trx_train = df_trx[lambda x: x['user_id'].isin(df_matching_train['bank'].values)]\n",
    "df_trx_valid = df_trx[lambda x: x['user_id'].isin(df_matching_valid['bank'].values)]\n",
    "df_trx_test = df_trx[lambda x: x['user_id'].isin(df_matching_test['bank'].values)]\n",
    "\n",
    "print(df_trx_train.shape, df_trx_valid.shape, df_trx_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb238769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((234420, 1563), (58600, 1563))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_dataset(df_matching, df_trx, df_click, neg_count=19):\n",
    "    positive = pd.merge(df_matching, df_trx, left_on='bank', right_on='user_id').drop(columns='user_id')\n",
    "    positive = pd.merge(positive, df_click, left_on='rtk', right_on='user_id').drop(columns='user_id')\n",
    "    positive['target'] = 1\n",
    "\n",
    "    rtks = df_click['user_id'].unique()\n",
    "\n",
    "    negative = pd.DataFrame(data=df_trx['user_id'].values, columns=['bank'])\n",
    "    negative['rtk'] = negative['bank'].apply(lambda x: np.random.choice(rtks, size=neg_count))\n",
    "    negative = negative.explode('rtk')\n",
    "\n",
    "    negative = pd.merge(negative, df_trx, left_on='bank', right_on='user_id').drop(columns='user_id')\n",
    "    negative = pd.merge(negative, df_click, left_on='rtk', right_on='user_id').drop(columns='user_id')\n",
    "    negative['target'] = 0\n",
    "\n",
    "    dataset = pd.concat([positive, negative]).sample(frac=1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train = prepare_dataset(df_matching_train, df_trx_train, df_click_train)\n",
    "valid = prepare_dataset(df_matching_valid, df_trx_valid, df_click_valid)\n",
    "\n",
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8086f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1aec26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ac2a8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikita/.local/share/virtualenvs/pytorch-lifestream-VR0LSEAW/lib/python3.8/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/nikita/.local/share/virtualenvs/pytorch-lifestream-VR0LSEAW/lib/python3.8/site-packages/lightgbm/basic.py:1702: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttraining's auc: 0.816927\tvalid_1's auc: 0.689003\n",
      "[100]\ttraining's auc: 0.841256\tvalid_1's auc: 0.699088\n",
      "[150]\ttraining's auc: 0.860464\tvalid_1's auc: 0.706953\n",
      "[200]\ttraining's auc: 0.877574\tvalid_1's auc: 0.715476\n",
      "[250]\ttraining's auc: 0.890523\tvalid_1's auc: 0.721447\n",
      "[300]\ttraining's auc: 0.903704\tvalid_1's auc: 0.725865\n",
      "[350]\ttraining's auc: 0.914664\tvalid_1's auc: 0.729821\n",
      "[400]\ttraining's auc: 0.925555\tvalid_1's auc: 0.733962\n",
      "[450]\ttraining's auc: 0.933285\tvalid_1's auc: 0.737244\n",
      "[500]\ttraining's auc: 0.941992\tvalid_1's auc: 0.740424\n",
      "[550]\ttraining's auc: 0.948728\tvalid_1's auc: 0.74348\n",
      "[600]\ttraining's auc: 0.955663\tvalid_1's auc: 0.746621\n",
      "[650]\ttraining's auc: 0.960148\tvalid_1's auc: 0.749322\n",
      "[700]\ttraining's auc: 0.964726\tvalid_1's auc: 0.751973\n",
      "[750]\ttraining's auc: 0.968581\tvalid_1's auc: 0.755053\n",
      "[800]\ttraining's auc: 0.971697\tvalid_1's auc: 0.757019\n",
      "[850]\ttraining's auc: 0.974734\tvalid_1's auc: 0.759195\n",
      "[900]\ttraining's auc: 0.977509\tvalid_1's auc: 0.761886\n",
      "[950]\ttraining's auc: 0.979722\tvalid_1's auc: 0.763665\n",
      "[1000]\ttraining's auc: 0.981657\tvalid_1's auc: 0.765053\n",
      "[1050]\ttraining's auc: 0.98334\tvalid_1's auc: 0.766073\n",
      "[1100]\ttraining's auc: 0.985258\tvalid_1's auc: 0.767089\n",
      "[1150]\ttraining's auc: 0.986597\tvalid_1's auc: 0.768266\n",
      "[1200]\ttraining's auc: 0.98801\tvalid_1's auc: 0.76912\n",
      "[1250]\ttraining's auc: 0.989023\tvalid_1's auc: 0.770228\n",
      "[1300]\ttraining's auc: 0.989965\tvalid_1's auc: 0.771142\n",
      "[1350]\ttraining's auc: 0.990807\tvalid_1's auc: 0.771672\n",
      "[1400]\ttraining's auc: 0.991699\tvalid_1's auc: 0.773025\n",
      "[1450]\ttraining's auc: 0.992467\tvalid_1's auc: 0.773842\n",
      "[1500]\ttraining's auc: 0.993104\tvalid_1's auc: 0.775018\n",
      "[1550]\ttraining's auc: 0.993717\tvalid_1's auc: 0.775874\n",
      "[1600]\ttraining's auc: 0.994234\tvalid_1's auc: 0.776344\n",
      "[1650]\ttraining's auc: 0.994682\tvalid_1's auc: 0.777053\n",
      "[1700]\ttraining's auc: 0.995123\tvalid_1's auc: 0.777747\n",
      "[1750]\ttraining's auc: 0.995593\tvalid_1's auc: 0.778562\n",
      "[1800]\ttraining's auc: 0.995939\tvalid_1's auc: 0.77903\n",
      "[1850]\ttraining's auc: 0.99625\tvalid_1's auc: 0.779379\n",
      "[1900]\ttraining's auc: 0.996547\tvalid_1's auc: 0.77969\n",
      "[1950]\ttraining's auc: 0.996788\tvalid_1's auc: 0.779834\n",
      "[2000]\ttraining's auc: 0.997037\tvalid_1's auc: 0.780374\n",
      "[2050]\ttraining's auc: 0.997285\tvalid_1's auc: 0.780704\n",
      "[2100]\ttraining's auc: 0.997499\tvalid_1's auc: 0.781164\n",
      "[2150]\ttraining's auc: 0.997674\tvalid_1's auc: 0.781678\n",
      "[2200]\ttraining's auc: 0.997853\tvalid_1's auc: 0.7819\n",
      "[2250]\ttraining's auc: 0.998036\tvalid_1's auc: 0.78234\n",
      "[2300]\ttraining's auc: 0.998176\tvalid_1's auc: 0.78256\n",
      "[2350]\ttraining's auc: 0.998307\tvalid_1's auc: 0.782766\n",
      "[2400]\ttraining's auc: 0.998428\tvalid_1's auc: 0.782935\n",
      "[2450]\ttraining's auc: 0.998549\tvalid_1's auc: 0.783282\n",
      "[2500]\ttraining's auc: 0.998664\tvalid_1's auc: 0.783631\n",
      "[2550]\ttraining's auc: 0.998754\tvalid_1's auc: 0.78362\n",
      "[2600]\ttraining's auc: 0.998855\tvalid_1's auc: 0.784165\n",
      "[2650]\ttraining's auc: 0.998928\tvalid_1's auc: 0.784298\n",
      "[2700]\ttraining's auc: 0.999015\tvalid_1's auc: 0.784513\n",
      "[2750]\ttraining's auc: 0.999077\tvalid_1's auc: 0.784871\n",
      "[2800]\ttraining's auc: 0.99914\tvalid_1's auc: 0.784687\n",
      "[2850]\ttraining's auc: 0.999215\tvalid_1's auc: 0.784648\n",
      "[2900]\ttraining's auc: 0.999268\tvalid_1's auc: 0.784727\n",
      "[2950]\ttraining's auc: 0.999319\tvalid_1's auc: 0.784899\n",
      "[3000]\ttraining's auc: 0.999364\tvalid_1's auc: 0.784966\n",
      "[3050]\ttraining's auc: 0.999404\tvalid_1's auc: 0.785001\n",
      "[3100]\ttraining's auc: 0.999444\tvalid_1's auc: 0.784968\n",
      "[3150]\ttraining's auc: 0.999479\tvalid_1's auc: 0.785047\n",
      "[3200]\ttraining's auc: 0.999517\tvalid_1's auc: 0.785309\n",
      "[3250]\ttraining's auc: 0.999546\tvalid_1's auc: 0.785262\n",
      "[3300]\ttraining's auc: 0.999574\tvalid_1's auc: 0.785328\n",
      "[3350]\ttraining's auc: 0.999602\tvalid_1's auc: 0.785493\n",
      "[3400]\ttraining's auc: 0.999626\tvalid_1's auc: 0.785757\n",
      "[3450]\ttraining's auc: 0.99965\tvalid_1's auc: 0.786041\n",
      "[3500]\ttraining's auc: 0.99967\tvalid_1's auc: 0.786201\n",
      "[3550]\ttraining's auc: 0.999688\tvalid_1's auc: 0.786174\n",
      "[3600]\ttraining's auc: 0.999707\tvalid_1's auc: 0.786193\n",
      "[3650]\ttraining's auc: 0.999722\tvalid_1's auc: 0.786365\n",
      "[3700]\ttraining's auc: 0.999738\tvalid_1's auc: 0.786354\n",
      "[3750]\ttraining's auc: 0.999751\tvalid_1's auc: 0.786487\n",
      "[3800]\ttraining's auc: 0.999764\tvalid_1's auc: 0.786482\n",
      "[3850]\ttraining's auc: 0.999777\tvalid_1's auc: 0.786519\n",
      "[3900]\ttraining's auc: 0.999789\tvalid_1's auc: 0.786427\n",
      "[3950]\ttraining's auc: 0.999801\tvalid_1's auc: 0.78627\n",
      "[4000]\ttraining's auc: 0.999811\tvalid_1's auc: 0.786294\n",
      "[4050]\ttraining's auc: 0.999821\tvalid_1's auc: 0.78635\n",
      "[4100]\ttraining's auc: 0.99983\tvalid_1's auc: 0.786411\n",
      "[4150]\ttraining's auc: 0.999838\tvalid_1's auc: 0.786395\n",
      "[4200]\ttraining's auc: 0.999844\tvalid_1's auc: 0.786477\n",
      "[4250]\ttraining's auc: 0.999852\tvalid_1's auc: 0.786439\n",
      "[4300]\ttraining's auc: 0.999859\tvalid_1's auc: 0.78645\n",
      "[4350]\ttraining's auc: 0.999865\tvalid_1's auc: 0.786789\n",
      "[4400]\ttraining's auc: 0.999872\tvalid_1's auc: 0.787025\n",
      "[4450]\ttraining's auc: 0.999878\tvalid_1's auc: 0.787136\n",
      "[4500]\ttraining's auc: 0.999883\tvalid_1's auc: 0.787306\n",
      "[4550]\ttraining's auc: 0.999888\tvalid_1's auc: 0.787546\n",
      "[4600]\ttraining's auc: 0.999893\tvalid_1's auc: 0.787698\n",
      "[4650]\ttraining's auc: 0.999898\tvalid_1's auc: 0.787766\n",
      "[4700]\ttraining's auc: 0.999902\tvalid_1's auc: 0.787804\n",
      "[4750]\ttraining's auc: 0.999906\tvalid_1's auc: 0.787992\n",
      "[4800]\ttraining's auc: 0.99991\tvalid_1's auc: 0.787948\n",
      "[4850]\ttraining's auc: 0.999913\tvalid_1's auc: 0.787976\n",
      "[4900]\ttraining's auc: 0.999917\tvalid_1's auc: 0.78801\n",
      "[4950]\ttraining's auc: 0.999921\tvalid_1's auc: 0.788081\n",
      "[5000]\ttraining's auc: 0.999924\tvalid_1's auc: 0.788151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7882"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "drop_columns = ['bank', 'rtk', 'target']\n",
    "TARGET = 'target'\n",
    "CAT_FEATURES = []\n",
    "\n",
    "params = dict(\n",
    "    objective='binary',\n",
    "    metric='auc',\n",
    "    n_estimators=5000,\n",
    "    boosting_type='gbdt',\n",
    "    learning_rate=0.01,\n",
    "    subsample=0.75,\n",
    "    subsample_freq=1,\n",
    "    feature_fraction=0.75,\n",
    "    max_depth=8,\n",
    "    lambda_l1=0.5,\n",
    "    lambda_l2=0.5,\n",
    "    num_leaves=128,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "train_data = lgb.Dataset(train.drop(columns=drop_columns),\n",
    "                                 label=train[TARGET],\n",
    "                                 categorical_feature=CAT_FEATURES)\n",
    "\n",
    "valid_data = lgb.Dataset(valid.drop(columns=drop_columns),\n",
    "                         label=valid[TARGET],\n",
    "                         categorical_feature=CAT_FEATURES)\n",
    "\n",
    "lgb_model = lgb.train(params, train_data, valid_sets=[train_data, valid_data], verbose_eval=50)\n",
    "preds = lgb_model.predict(valid.drop(columns=drop_columns))\n",
    "\n",
    "metric_value = roc_auc_score(valid[TARGET], preds)\n",
    "round(metric_value, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fda5256",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trx_valid.set_index('user_id', inplace=True)\n",
    "df_click_valid.set_index('user_id', inplace=True)\n",
    "df_matching_valid.set_index('bank', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "501e0751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(df_trx, df_click, model, model_features, batch_size=200):\n",
    "\n",
    "    list_of_rtk = list(df_click.index.unique())\n",
    "    list_of_bank = list(df_trx.index.unique())\n",
    "\n",
    "    submission = pd.DataFrame(list_of_bank, columns=['bank'])\n",
    "    submission['rtk'] = submission['bank'].apply(lambda x: list_of_rtk)\n",
    "\n",
    "    num_of_batches = int((len(list_of_bank))/batch_size)+1\n",
    "    submission_ready = []\n",
    "\n",
    "    for i in range(num_of_batches):\n",
    "        bank_ids = list_of_bank[(i*batch_size):((i+1)*batch_size)]\n",
    "        if len(bank_ids) != 0:\n",
    "            part_of_submit = submission[submission['bank'].isin(bank_ids)].explode('rtk')\n",
    "            part_of_submit = part_of_submit.merge(df_trx, how='left', left_on='bank', right_index=True\n",
    "                                        ).merge(df_click, how='left', left_on='rtk', right_index=True).fillna(0)\n",
    "\n",
    "            for feature in model_features:\n",
    "                if feature not in part_of_submit.columns:\n",
    "                    part_of_submit[feature] = 0\n",
    "\n",
    "            part_of_submit['predicts'] = model.predict(part_of_submit[model_features])\n",
    "            part_of_submit = part_of_submit[['bank', 'rtk', 'predicts']]\n",
    "\n",
    "            part_of_submit = part_of_submit.sort_values(by=['bank', 'predicts'], ascending=False).reset_index(drop=True)\n",
    "            part_of_submit = part_of_submit.pivot_table(index='bank', values='rtk', aggfunc=list)\n",
    "            part_of_submit['rtk'] = part_of_submit['rtk'].apply(lambda x: x[:100])\n",
    "            part_of_submit['bank'] = part_of_submit.index\n",
    "            part_of_submit = part_of_submit[['bank', 'rtk']]\n",
    "            submission_ready.extend(part_of_submit.values)\n",
    "\n",
    "    submission_final = np.array(submission_ready, dtype=object)\n",
    "    return submission_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f111a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 33min 16s, sys: 39 s, total: 1h 33min 55s\n",
      "Wall time: 13min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preds = inference(df_trx_valid, df_click_valid, lgb_model, lgb_model.feature_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5f48926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre: 0.360, mrr: 0.175, r1: 0.235\n",
      "CPU times: user 102 ms, sys: 8 µs, total: 102 ms\n",
      "Wall time: 101 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def score(preds, matching):\n",
    "    pre = 0.0\n",
    "    mrr = 0.0\n",
    "\n",
    "    for ix_bank, pred in preds:\n",
    "        match = matching.loc[ix_bank]['rtk']\n",
    "        d = match in pred\n",
    "        if d:\n",
    "            pre += 1\n",
    "            mrr += 1 / (1 + pred.index(match))\n",
    "\n",
    "    pre /= len(preds)\n",
    "    mrr /= len(preds)\n",
    "    r1 = 2 * pre * mrr / (pre + mrr)\n",
    "\n",
    "    return pre, mrr, r1\n",
    "\n",
    "pre, mrr, r1 = score(preds, df_matching_valid)\n",
    "\n",
    "print(f'pre: {pre:.3f}, mrr: {mrr:.3f}, r1: {r1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "016634c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('lgb_submit/lgb_model.p', 'wb') as f:\n",
    "    pickle.dump(lgb_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94d008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba6453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
