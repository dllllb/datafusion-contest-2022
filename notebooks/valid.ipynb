{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "simplified-country",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/kireev/pycharm-deploy/vtb\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-kansas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "restricted-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "subtle-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "oriented-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "latin-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "numeric-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.data_preprocessing.pandas_preprocessor import PandasDataPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "labeled-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhocon import ConfigFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-upper",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "accepted-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD_ID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "under-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_id_test = FOLD_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fifty-waste",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds_count = len(glob('data/train_matching_*.csv'))\n",
    "folds_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "suburban-detector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fold_id_valid = np.random.choice([i for i in range(folds_count) if i != fold_id_test], size=1)[0]\n",
    "fold_id_valid = (fold_id_test + 1) % folds_count\n",
    "fold_id_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "confidential-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_matching_train = pd.concat([pd.read_csv(f'data/train_matching_{i}.csv')\n",
    "#                               for i in range(folds_count) \n",
    "#                               if i not in (fold_id_test, fold_id_valid)])\n",
    "df_matching_valid = pd.read_csv(f'data/train_matching_{fold_id_valid}.csv')\n",
    "df_matching_test = pd.read_csv(f'data/train_matching_{fold_id_test}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "smooth-experiment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2930, 2930]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(df) for df in [df_matching_valid, df_matching_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "related-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trx_types(df):\n",
    "    df['mcc_code'] = df['mcc_code'].astype(str)\n",
    "    df['currency_rk'] = df['currency_rk'].astype(str)\n",
    "    df['event_time'] = pd.to_datetime(df['transaction_dttm']).astype(int) / 1e9\n",
    "    return df[['user_id', 'event_time', 'mcc_code', 'currency_rk', 'transaction_amt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "helpful-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_trx_train = pd.concat([trx_types(pd.read_csv(f'data/transactions_{i}.csv'))\n",
    "#                               for i in range(folds_count) \n",
    "#                               if i not in (fold_id_test, fold_id_valid)])\n",
    "df_trx_valid = trx_types(pd.read_csv(f'data/transactions_{fold_id_valid}.csv'))\n",
    "# df_trx_test = trx_types(pd.read_csv(f'data/transactions_{fold_id_test}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "raised-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_types(df):\n",
    "    df['event_time'] = pd.to_datetime(df['timestamp']).astype(int) / 1e9\n",
    "    df = pd.merge(df, pd.read_csv('data/click_categories.csv'), on='cat_id')\n",
    "    df['cat_id'] = df['cat_id'].astype(str)\n",
    "    return df[['user_id', 'event_time', 'cat_id', 'level_0', 'level_1', 'level_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "worse-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_click_train = pd.concat([click_types(pd.read_csv(f'data/clickstream_{i}.csv'))\n",
    "#                               for i in range(folds_count) \n",
    "#                               if i not in (fold_id_test, fold_id_valid)])\n",
    "df_click_valid = click_types(pd.read_csv(f'data/clickstream_{fold_id_valid}.csv'))\n",
    "# df_click_test = click_types(pd.read_csv(f'data/clickstream_{fold_id_test}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-penalty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "together-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('preprocessor_trx.p', 'rb') as f:\n",
    "    preprocessor_trx = pickle.load(f)\n",
    "with open('preprocessor_click.p', 'rb') as f:\n",
    "    preprocessor_click = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-campaign",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "monthly-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.data_load.iterable_processing.category_size_clip import CategorySizeClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "oriented-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_max_size_trx = {\n",
    "    'mcc_code': 350,\n",
    "    'currency_rk': 5,\n",
    "}\n",
    "category_max_size_click = {\n",
    "    'cat_id': 400,\n",
    "    'level_0': 400,\n",
    "    'level_1': 400,\n",
    "    'level_2': 400,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "affiliated-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trx_to_torch(seq):\n",
    "    seq = CategorySizeClip(category_max_size_trx)(seq)\n",
    "    for x in seq:\n",
    "        yield x['user_id'], {\n",
    "            'event_time': torch.from_numpy(x['event_time']).float(),\n",
    "            'mcc_code': torch.from_numpy(x['mcc_code']).int(),\n",
    "            'currency_rk': torch.from_numpy(x['currency_rk']).int(),\n",
    "            'transaction_amt': torch.from_numpy(x['transaction_amt']).float(),\n",
    "        }\n",
    "\n",
    "def click_to_torch(seq):\n",
    "    seq = CategorySizeClip(category_max_size_click)(seq)\n",
    "    for x in seq:\n",
    "        yield x['user_id'], {\n",
    "            'event_time': torch.from_numpy(x['event_time']).float(),\n",
    "            'cat_id': torch.from_numpy(x['cat_id']).int(),\n",
    "            'level_0': torch.from_numpy(x['level_0']).int(),\n",
    "            'level_1': torch.from_numpy(x['level_1']).int(),\n",
    "            'level_2': torch.from_numpy(x['level_2']).int(),\n",
    "\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "transsexual-shepherd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: cat_id, dtype: int64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_click_valid[lambda x: x['user_id'].eq('958c5752b04342c796e38d523816760c')]['cat_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "rural-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_transform(self, df, copy=True):\n",
    "    \"\"\"Perform preprocessing.\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame with flat data\n",
    "    copy : bool, default=None\n",
    "        Copy the input X or not.\n",
    "    Returns\n",
    "    -------\n",
    "    features : List of dicts grouped by col_id.\n",
    "    \"\"\"\n",
    "        self.check_is_fitted()\n",
    "        df_data = df.copy() if copy else df\n",
    "\n",
    "        # event_time mapping\n",
    "        if self.time_transformation == 'none':\n",
    "            pass\n",
    "        elif self.time_transformation == 'default':\n",
    "            df_data = self._td_default(df_data, self.cols_event_time)\n",
    "        elif self.time_transformation == 'float':\n",
    "            df_data = self._td_float(df_data, self.cols_event_time)\n",
    "        elif self.time_transformation == 'gender':\n",
    "            df_data = self._td_gender(df_data, self.cols_event_time)\n",
    "        else:\n",
    "            raise NotImplementedError(f'Unknown type of data transformation: \"{self.time_transformation}\"')\n",
    "\n",
    "        for col in self.cols_category:\n",
    "            if col not in self.cols_category_mapping:\n",
    "                raise KeyError(f\"column {col} isn't in fitted category columns\")\n",
    "            pd_col = df_data[col].astype(str)\n",
    "            df_data[col] = pd_col.map(self.cols_category_mapping[col]) \\\n",
    "                .fillna(max(self.cols_category_mapping[col].values()))\n",
    "\n",
    "        for col in self.cols_log_norm:\n",
    "            df_data[col] = np.log1p(abs(df_data[col])) * np.sign(df_data[col])\n",
    "            df_data[col] /= abs(df_data[col]).max()\n",
    "\n",
    "        if self.print_dataset_info:\n",
    "            df = df_data.groupby(self.col_id)['event_time'].count()\n",
    "\n",
    "        # column filter\n",
    "        used_columns = [col for col in df_data.columns\n",
    "                        if col in self.cols_category + self.cols_log_norm + ['event_time', self.col_id]]\n",
    "\n",
    "        features = df_data[used_columns] \\\n",
    "            .assign(et_index=lambda x: x['event_time']) \\\n",
    "            .set_index([self.col_id, 'et_index']).sort_index() \\\n",
    "            .groupby(self.col_id).apply(lambda x: {k: np.array(v) for k, v in x.to_dict(orient='list').items()}) \\\n",
    "            .rename('feature_arrays').reset_index().to_dict(orient='records')\n",
    "\n",
    "        def squeeze(rec):\n",
    "            return {self.col_id: rec[self.col_id], **rec['feature_arrays']}\n",
    "        features = [squeeze(r) for r in features]\n",
    "\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "import types\n",
    "preprocessor_trx.transform = types.MethodType(new_transform, preprocessor_trx)\n",
    "preprocessor_click.transform = types.MethodType(new_transform, preprocessor_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "upset-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_trx_train = dict(trx_to_torch(preprocessor_trx.fit_transform(df_trx_train)))\n",
    "features_trx_valid = dict(trx_to_torch(preprocessor_trx.transform(df_trx_valid)))\n",
    "# features_trx_test = dict(trx_to_torch(preprocessor_trx.transform(df_trx_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "horizontal-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_click_train = dict(click_to_torch(preprocessor_click.fit_transform(df_click_train)))\n",
    "features_click_valid = dict(click_to_torch(preprocessor_click.transform(df_click_valid)))\n",
    "# features_click_test = dict(click_to_torch(preprocessor_click.transform(df_click_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-civilian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "latter-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vtb_code.data import PairedDataset, CrossDataset, paired_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "digital-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.data_load import augmentation_chain\n",
    "from dltranz.data_load.augmentations.seq_len_limit import SeqLenLimit\n",
    "from dltranz.data_load.augmentations.random_slice import RandomSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "continental-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RandomSample:\n",
    "    def __init__(self, min_len, max_len, rate_for_min=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.min_len = min_len\n",
    "        self.max_len = max_len\n",
    "        self.rate_for_min = rate_for_min\n",
    "\n",
    "    def __call__(self, x):\n",
    "        seq_len = len(next(iter(x.values())))\n",
    "\n",
    "        idx = self.get_idx(seq_len)\n",
    "        new_x = {k: v[idx] for k, v in x.items()}\n",
    "        return new_x\n",
    "\n",
    "    def get_idx(self, seq_len):\n",
    "        new_idx = np.arange(seq_len)\n",
    "\n",
    "        min_len, max_len = self.get_min_max(seq_len)\n",
    "        if max_len < min_len:\n",
    "            return new_idx\n",
    "        new_len = random.randint(min_len, max_len)\n",
    "\n",
    "        return np.sort(np.random.choice(new_idx, size=new_len, replace=False))\n",
    "\n",
    "    def get_min_max(self, seq_len):\n",
    "        max_len = int(min(self.max_len, seq_len))\n",
    "        min_len = int(min(self.min_len, seq_len * self.rate_for_min))\n",
    "        if min_len < 1:\n",
    "            min_len = 1\n",
    "        return min_len, max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "informed-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset0 = CrossDataset(\n",
    "    [\n",
    "        df_matching_valid['bank'].unique(),\n",
    "    ], \n",
    "    data=[\n",
    "        features_trx_valid,\n",
    "    ],\n",
    "    augmentations=[\n",
    "        augmentation_chain(SeqLenLimit(2000)),\n",
    "    ],\n",
    ")\n",
    "valid_dataset1 = CrossDataset(\n",
    "    [\n",
    "        df_matching_valid['rtk'][lambda x: x.ne('0')].unique(),\n",
    "    ], \n",
    "    data=[\n",
    "        features_click_valid,\n",
    "    ],\n",
    "    augmentations=[\n",
    "        augmentation_chain(SeqLenLimit(5000)),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "middle-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl0 = torch.utils.data.DataLoader(\n",
    "    valid_dataset0,\n",
    "    collate_fn=paired_collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    batch_size=512,\n",
    ")\n",
    "valid_dl1 = torch.utils.data.DataLoader(\n",
    "    valid_dataset1,\n",
    "    collate_fn=paired_collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    batch_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-location",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "crude-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.seq_encoder import create_encoder\n",
    "from dltranz.metric_learn.sampling_strategies import get_sampling_strategy\n",
    "from dltranz.metric_learn.losses import get_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "productive-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vtb_code.metrics import PrecisionK, MeanReciprocalRankK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-defeat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "lesbian-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedModule(pl.LightningModule):\n",
    "    def __init__(self, params, neg_count, k,\n",
    "                 lr, weight_decay,\n",
    "                 step_size, gamma,\n",
    "                 base_lr, max_lr, step_size_up, step_size_down,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['params', 'loss_fn', 'loss_w'])\n",
    "        \n",
    "        self.seq_encoder_trx = torch.nn.Sequential(\n",
    "            create_encoder(params['trx_seq'], is_reduce_sequence=True),\n",
    "#             torch.nn.Linear(params['trx_seq.rnn.hidden_size'], params['head_size']),\n",
    "        )\n",
    "        self.seq_encoder_click = torch.nn.Sequential(\n",
    "            create_encoder(params['click_seq'], is_reduce_sequence=True),\n",
    "#             torch.nn.Linear(params['click_seq.rnn.hidden_size'], params['head_size']),\n",
    "        )\n",
    "                \n",
    "        self.train_precision = PrecisionK(k=k, compute_on_step=False)\n",
    "        self.train_mrr = MeanReciprocalRankK(k=k, compute_on_step=False)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        if self.hparams.step_size is not None:\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                optim, step_size=self.hparams.step_size, gamma=self.hparams.gamma)\n",
    "        else:\n",
    "            sheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "                optim,\n",
    "                base_lr=self.hparams.base_lr, max_lr=self.hparams.max_lr,\n",
    "                step_size_up=self.hparams.step_size_up,\n",
    "                step_size_down=self.hparams.step_size_down,\n",
    "                cycle_momentum=False,\n",
    "            )\n",
    "            scheduler = {'scheduler': sheduler, 'interval': 'step'}\n",
    "        return [optim], [scheduler]\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        x_trx, x_click = batch\n",
    "        z_trx = self.seq_encoder_trx(x_trx)  # B, H\n",
    "        z_click = self.seq_encoder_click(x_click)  # B, H\n",
    "        \n",
    "        B, H = z_trx.size()\n",
    "        logits = (z_trx * z_click).sum(dim=1)  # B(trx),\n",
    "        return logits\n",
    "    \n",
    "    def pos_pairs(self, a, b):\n",
    "        l2 = torch.nn.functional.pairwise_distance(a, b)  # B(trx),\n",
    "        return l2\n",
    "    \n",
    "    def neg_pairs(self, a, b):\n",
    "        l2 = torch.nn.functional.pairwise_distance(a.unsqueeze(1), b.unsqueeze(0))  # B(trx) * B(click)\n",
    "        B, _ = l2.size()\n",
    "        device = l2.device\n",
    "        neg_mask = torch.eye(B, device=device) == 1\n",
    "        l2 = l2.masked_fill(neg_mask, 10)\n",
    "        k = min(self.hparams.neg_count, B - 1)\n",
    "        neg_pairs = torch.topk(l2, k=k, dim=1, largest=False).values\n",
    "        return neg_pairs\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_trx, x_click = batch\n",
    "        z_trx = self.seq_encoder_trx(x_trx)  # B, H\n",
    "        z_click = self.seq_encoder_click(x_click)  # B, H\n",
    "\n",
    "        z_trx = torch.nn.functional.normalize(z_trx, dim=1)\n",
    "        z_click = torch.nn.functional.normalize(z_click, dim=1)\n",
    "       \n",
    "        with torch.no_grad():\n",
    "            l2 = torch.nn.functional.pairwise_distance(z_trx.unsqueeze(1), z_click.unsqueeze(0))\n",
    "            # B(trx) * B(click)\n",
    "            self.train_precision(-l2)\n",
    "            self.train_mrr(-l2)\n",
    "        \n",
    "        B, H = z_trx.size()\n",
    "        loss_sum, cnt = 0.0, 0\n",
    "        res = self.pos_pairs(z_trx, z_click)\n",
    "        self.log('logits_metrics/l2_pos_trx_click', res.mean())\n",
    "        loss_sum += res.pow(2).sum()\n",
    "        cnt += res.size(0)\n",
    "        \n",
    "        res = self.neg_pairs(z_trx, z_trx)\n",
    "        self.log('logits_metrics/l2_neg_trx_trx', res.mean())\n",
    "        loss_sum += torch.relu(0.5 - res).pow(2).sum()\n",
    "        cnt += res.size(0) * res.size(1)\n",
    "\n",
    "        res = self.neg_pairs(z_trx, z_click)\n",
    "        self.log('logits_metrics/l2_neg_trx_click', res.mean())\n",
    "        loss_sum += torch.relu(0.5 - res).pow(2).sum()\n",
    "        cnt += res.size(0) * res.size(1)\n",
    "\n",
    "        res = self.neg_pairs(z_click, z_click)\n",
    "        self.log('logits_metrics/l2_neg_click_click', res.mean())\n",
    "        loss_sum += torch.relu(0.5 - res).pow(2).sum()\n",
    "        cnt += res.size(0) * res.size(1)\n",
    "\n",
    "        loss = loss_sum / cnt\n",
    "        self.log('loss/l2', res.mean())\n",
    "        return loss\n",
    "        \n",
    "    def training_epoch_end(self, _):\n",
    "        self.log('train_metrics/precision', self.train_precision, prog_bar=True)\n",
    "        self.log('train_metrics/mrr', self.train_mrr, prog_bar=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "removable-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "params =     ConfigFactory.parse_string('''\n",
    "    trx_seq: {\n",
    "        trx_encoder: {\n",
    "          use_batch_norm_with_lens: false\n",
    "          norm_embeddings: false,\n",
    "          embeddings_noise: 0.000,\n",
    "          embeddings: {\n",
    "            mcc_code: {in: 350, out: 64},\n",
    "            currency_rk: {in: 10, out: 4}\n",
    "          },\n",
    "          numeric_values: {\n",
    "            transaction_amt: identity\n",
    "          }\n",
    "        },\n",
    "        encoder_type: rnn,\n",
    "        rnn: {\n",
    "          type: gru,\n",
    "          hidden_size: 128,\n",
    "          bidir: false,\n",
    "          trainable_starter: static\n",
    "        }\n",
    "    }\n",
    "    click_seq: {\n",
    "        trx_encoder: {\n",
    "          use_batch_norm_with_lens: false\n",
    "          norm_embeddings: false,\n",
    "          embeddings_noise: 0.000,\n",
    "          embeddings: {\n",
    "            cat_id: {in: 400, out: 64},\n",
    "            # level_0: {in: 400, out: 16}\n",
    "            # level_1: {in: 400, out: 8}\n",
    "            # level_2: {in: 400, out: 4}\n",
    "          },\n",
    "          numeric_values: {\n",
    "          }\n",
    "        },\n",
    "        encoder_type: rnn,\n",
    "        rnn: {\n",
    "          type: gru,\n",
    "          hidden_size: 128,\n",
    "          bidir: false,\n",
    "          trainable_starter: static\n",
    "        }    \n",
    "    }\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "black-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_model = PairedModule.load_from_checkpoint('lightning_logs/version_15/checkpoints/epoch=2999-step=29999.ckpt',\n",
    "                                              params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "express-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "illegal-zoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_model.eval()\n",
    "sup_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "thick-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-plymouth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "coastal-newman",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95dfb62855b948d49cad436879a7deac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d11940092db4bc4ac51a4f80af7522c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_valid0 = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(valid_dl0):\n",
    "        z_trx = sup_model.seq_encoder_trx(batch[0].to(device))  # B, H\n",
    "        z_trx = torch.nn.functional.normalize(z_trx, dim=1)\n",
    "        m_valid0.append(z_trx.cpu())\n",
    "\n",
    "\n",
    "m_valid1 = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(valid_dl1):\n",
    "        z_click = sup_model.seq_encoder_click(batch[0].to(device))  # B, H\n",
    "        z_click = torch.nn.functional.normalize(z_click, dim=1)\n",
    "        m_valid1.append(z_click.cpu())\n",
    "\n",
    "\n",
    "m_valid = pd.DataFrame(\n",
    "    torch.nn.functional.pairwise_distance(\n",
    "        torch.cat(m_valid0, axis=0).unsqueeze(1),\n",
    "        torch.cat(m_valid1, axis=0).unsqueeze(0),\n",
    "    ).numpy(), \n",
    "    index=valid_dataset0.unique_ids[0],\n",
    "    columns=valid_dataset1.unique_ids[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-prior",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-idaho",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-arnold",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "democratic-excuse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629a70e2252944949f934fd9c52dc0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pre = 0.0\n",
    "mrr = 0.0\n",
    "\n",
    "for ix_bank, s_scores in tqdm(m_valid.iterrows()):\n",
    "    d = s_scores.sort_values().iloc[:100].rename('l2').reset_index().rename(columns={'index': 'rtk'}) \\\n",
    "    .assign(rank=1 / (np.arange(100) + 1))[lambda x: x['rtk'].eq(\n",
    "        df_matching_valid.set_index('bank').loc[ix_bank, 'rtk'])]\n",
    "    if len(d) == 1:\n",
    "        pre += 1\n",
    "        mrr += d['rank'].iloc[0]\n",
    "pre = pre / len(m_valid)\n",
    "mrr = mrr / len(m_valid)\n",
    "r1 = 2 * pre * mrr / (pre + mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "divided-attachment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre: 0.078, mrr: 0.005, r1: 0.010\n"
     ]
    }
   ],
   "source": [
    "print(f'pre: {pre:.3f}, mrr: {mrr:.3f}, r1: {r1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "controlling-memorial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73fcdabc690f4b739d4fffd9fbc240e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pre = 0.0\n",
    "mrr = 0.0\n",
    "\n",
    "for ix_bank, s_scores in tqdm(\n",
    "    m_valid.iloc[:, np.linspace(0, m_valid.shape[1] - 1, 1024).astype(int)].iterrows()):\n",
    "    d = s_scores.sort_values().iloc[:34].rename('l2').reset_index().rename(columns={'index': 'rtk'}) \\\n",
    "    .assign(rank=1 / (np.arange(34) + 1))[lambda x: x['rtk'].eq(\n",
    "        df_matching_valid.set_index('bank').loc[ix_bank, 'rtk'])]\n",
    "    if len(d) == 1:\n",
    "        pre += 1\n",
    "        mrr += d['rank'].iloc[0]\n",
    "pre = pre / len(m_valid)\n",
    "mrr = mrr / len(m_valid)\n",
    "r1 = 2 * pre * mrr / (pre + mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "duplicate-delight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre: 0.025, mrr: 0.003, r1: 0.006\n"
     ]
    }
   ],
   "source": [
    "print(f'pre: {pre:.3f}, mrr: {mrr:.3f}, r1: {r1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "chronic-falls",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.13333333333333"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 / 3000 * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-shaft",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-ordinary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-exhibition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "small-ceramic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre: 0.051, mrr: 0.003, r1: 0.005\n"
     ]
    }
   ],
   "source": [
    "print(f'pre: {pre:.3f}, mrr: {mrr:.3f}, r1: {r1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-program",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-spell",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vtb",
   "language": "python",
   "name": "vtb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
